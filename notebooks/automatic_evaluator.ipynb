{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "import torch\n",
    "def set_css():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dotenv was True\n",
      "dotenv was True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'iesta' from 'c:\\\\Users\\\\elba_ro\\\\Documents\\\\projects\\\\github\\\\conf22-style-transfer\\\\iesta\\\\__init__.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from iesta.llms.generate import Generator\n",
    "import argparse\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "import iesta\n",
    "import iesta.llms\n",
    "import iesta.llms.models\n",
    "from iesta.llms.models import LlamaV2, ChatGpt\n",
    "\n",
    "import importlib\n",
    "importlib.reload(iesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import simpletransformers\n",
    "from simpletransformers.ner import NERModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = processed_toxicity_dict['liberal_llamav2_0shot_steered_medianl0.2_processed.csv']\n",
    "# sns.countplot(data=df, x=\"success\", hue=\"toxic_str\")\n",
    "counter_arr = []\n",
    "for k, experiment in processed_toxicity_dict.items():\n",
    "    # exp_res = {\"experiment\": experiment}\n",
    "    \n",
    "    exp_res =experiment[[\"success\", \"toxic_str\"]].groupby([\"success\", \"toxic_str\"])[\"toxic_str\"].agg([ \"count\"]).add_prefix('toxicity_').reset_index()\n",
    "    exp_res[\"experiment\"] = k.replace(\".csv\", \"\")\n",
    "    exp_res[\"ideology\"] = k.split(\"_\")[0]\n",
    "    exp_res[\"model\"] = k.split(\"_\")[1]\n",
    "    exp_res[\"Shot\"] = k.split(\"_\")[2]\n",
    "    \n",
    "    counter_arr.append(exp_res)\n",
    "\n",
    "\n",
    "def _apply_format(row):\n",
    "    row[\"success_toxic\"] = f\"{'Response' if row['success'] else 'No Response'} w {row['toxic_str'].replace('Not ', 'Non-')} Argument\"\n",
    "    return row\n",
    "concat_count_df = pd.concat(counter_arr, ignore_index=True, )\n",
    "concat_count_df = concat_count_df.apply(_apply_format, axis=1)\n",
    "concat_count_df = concat_count_df[[\"success_toxic\", \"toxicity_count\", \"experiment\", \"model\"]]\n",
    "concat_count_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat_count_df.set_index(\"experiment\")\n",
    "ax = sns.barplot(concat_count_df, x=\"experiment\", y=\"toxicity_count\", hue=\"success_toxic\", dodge=True)#, col=\"model\")\n",
    "#ax.plot(1955, 3600, \"*\", markersize=10, color=\"r\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dotenv was True\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import iesta\n",
    "import iesta.evaluator\n",
    "importlib.reload(iesta)\n",
    "\n",
    "importlib.reload(iesta.evaluator)\n",
    "from iesta.evaluator import generation_processor, evaluator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from iesta.llms.generate import Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/llms_out/new\\*.jsonl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['conservative_chatgpt_0shot',\n",
       " 'conservative_chatgpt_1shot',\n",
       " 'conservative_llamav2_0shot',\n",
       " 'conservative_llamav2_0shot_steered_meanl0.2',\n",
       " 'conservative_llamav2_0shot_steered_meanl0.5',\n",
       " 'conservative_llamav2_0shot_steered_medianl0.2',\n",
       " 'conservative_llamav2_0shot_steered_medianl0.5',\n",
       " 'conservative_llamav2_1shot',\n",
       " 'liberal_chatgpt_0shot',\n",
       " 'liberal_chatgpt_1shot',\n",
       " 'liberal_llamav2_0shot',\n",
       " 'liberal_llamav2_0shot_steered_meanl0.2',\n",
       " 'liberal_llamav2_0shot_steered_meanl0.5',\n",
       " 'liberal_llamav2_0shot_steered_medianl0.2',\n",
       " 'liberal_llamav2_0shot_steered_medianl0.5',\n",
       " 'liberal_llamav2_1shot']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments = generation_processor._fetch_all_experiments(root=\"\")\n",
    "experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = experiment_eval_dict[\"conservative_llamav2_1shot\"].successful_df\n",
    "df_ = df_[~df_[\"effective\"].isna()]\n",
    "df_[\"effective\"]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#pd.read_csv(\"data/llms_out/new/style_features/liwc_test_conservative_ineffective.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conservative_chatgpt_1shot\n",
      "1. postprocessing the generated arguments data/llms_out/new/conservative_chatgpt_1shot.jsonl...\n",
      "data/llms_out/new\\*.jsonl\n",
      "data/out/conservative_idx.csv\n",
      "original INDICESS found\n",
      "fetching original ineffective arguments...\n",
      "Getting Original Data...\n",
      "original file found\n",
      "2. Calculating the toxicity scores for Ineffective arguments - existing cols ['text', 'label', 'author', 'original_text', 'category', 'round', 'debate_id', 'idx', 'toxic']...\n",
      "Toxicity score already calculated for INPUT (ineffective data)\n",
      "-- merging original argument info with generated one\n",
      "-- merged data length: 3500 -> 3493 --> loss: 7\n",
      " ### Stats ### \n",
      "3.a adding has_ideology_prompt\n",
      "3.b setting count_success_per_prompt_df\n",
      "3.c setting count_success_per_category_df\n",
      "3.d setting count_type_per_prompt_df\n",
      "3.e setting count_success_per_ideology_prompt_df\n",
      "3.f setting count_success_per_toxic_df\n",
      "3.g setting top_ngrams_count_failed_response n=10, top=30\n",
      "3.h setting failed_ratio\n",
      "-- 0.0: 0 out of 3493\n",
      "3.i setting corr_toxicity_no_response\n",
      "4. ### Scoring style ### \n",
      "extracting style features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\elba_ro\\.pyenv\\pyenv-win\\versions\\3.11.2\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "c:\\Users\\elba_ro\\.pyenv\\pyenv-win\\versions\\3.11.2\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "nlpaf        INFO     adding pipe with name MpqaPipeOrchestrator\n",
      "nlpaf        INFO     orchestrator was initialized successfully\n",
      "nlpaf        INFO     adding pipe with code mpqa_arg_component\n",
      "nlpaf        INFO     Defining pipe default and spacy stacks\n",
      "nlpaf        INFO     Pipes are ['mpqa_parser', 'mpqa_arg_component']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotating...\n",
      "saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nlpaf        INFO     adding pipe with name EmotionPipeOrchestrator\n",
      "nlpaf        INFO     orchestrator was initialized successfully\n",
      "nlpaf        INFO     adding pipe with code emotion_hartmann_component\n",
      "nlpaf        INFO     adding pipe with name HedgePipeOrchestrator\n",
      "nlpaf        INFO     orchestrator was initialized successfully\n",
      "nlpaf        INFO     adding pipe with code hedge_component\n",
      "nlpaf        INFO     Defining pipe default and spacy stacks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved file length: 3493 and 3493\n",
      "extracting transformer features\n",
      "Initializing style features pipeline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\elba_ro\\.pyenv\\pyenv-win\\versions\\3.11.2\\Lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "c:\\Users\\elba_ro\\.pyenv\\pyenv-win\\versions\\3.11.2\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "nlpaf        INFO     Pipes are ['sentencizer', 'emotion_hartmann_component', 'hedge_component']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  should be 0)\n",
      "annotating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [05:56<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\elba_ro\\.pyenv\\pyenv-win\\versions\\3.11.2\\Lib\\multiprocessing\\pool.py:856\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 856\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[1;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m shot \u001b[38;5;241m=\u001b[39m splits[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshot\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m steered \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplits[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplits[\u001b[38;5;241m4\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(splits) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m4\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 14\u001b[0m experiment_eval_dict[experiment] \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEvaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mideology\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mideology\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mshot_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshot\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43msteered\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteered\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mroot_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#score[ideology] = experiment_eval_dict[experiment].score_style(is_for_ineffective=True) if ideology not in score else score[ideology]\u001b[39;00m\n",
      "File \u001b[1;32m<string>:9\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, model_type, ideology, shot_num, steered, feature_liwc_path, root_path)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\elba_ro\\Documents\\projects\\github\\conf22-style-transfer\\iesta\\evaluator\\evaluator.py:156\u001b[0m, in \u001b[0;36mEvaluator.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuccessful_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerged_df[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerged_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4. ### Scoring style ### \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstyle_score_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore_style\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m all_w_style_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerged_df,\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstyle_score_df,\n\u001b[0;32m    161\u001b[0m     left_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    162\u001b[0m     right_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    163\u001b[0m     how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m\"\u001b[39m,)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(all_w_style_df) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerged_df)\n",
      "File \u001b[1;32mc:\\Users\\elba_ro\\Documents\\projects\\github\\conf22-style-transfer\\iesta\\evaluator\\evaluator.py:570\u001b[0m, in \u001b[0;36mEvaluator.score_style\u001b[1;34m(self, is_for_ineffective, rerun_style_extraction)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscore_style\u001b[39m(\u001b[38;5;28mself\u001b[39m, is_for_ineffective\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    569\u001b[0m                 rerun_style_extraction: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m--> 570\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_style_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrerun_style_extraction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    571\u001b[0m     top_features: pd\u001b[38;5;241m.\u001b[39mDataFrame \u001b[38;5;241m=\u001b[39m get_top_features(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mideology, root\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_path)\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop feature \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(top_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\elba_ro\\Documents\\projects\\github\\conf22-style-transfer\\iesta\\evaluator\\evaluator.py:376\u001b[0m, in \u001b[0;36mEvaluator.extract_style_features\u001b[1;34m(self, rerun_style_extraction)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_style_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, rerun_style_extraction: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_basic_features(rerun_style_extraction)\n\u001b[1;32m--> 376\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_transformer_based_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrerun_style_extraction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\elba_ro\\Documents\\projects\\github\\conf22-style-transfer\\iesta\\evaluator\\evaluator.py:479\u001b[0m, in \u001b[0;36mEvaluator._extract_transformer_based_features\u001b[1;34m(self, rerun_style_extraction)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannotating...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 479\u001b[0m     \u001b[43mpipeline_transformer_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mannotate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaving...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    481\u001b[0m     pipeline_transformer_features\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nlpaf\\annotator\\pipeline\\pipeline_base.py:121\u001b[0m, in \u001b[0;36mPipeline.annotate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    119\u001b[0m _component \u001b[38;5;241m=\u001b[39m s[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomponent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m s[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspacy\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 121\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomponent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m            \u001b[49m\u001b[43mas_tuples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_process\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspacy_n_processors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m out\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\elba_ro\\.pyenv\\pyenv-win\\versions\\3.11.2\\Lib\\site-packages\\spacy\\language.py:1574\u001b[0m, in \u001b[0;36mLanguage.pipe\u001b[1;34m(self, texts, as_tuples, batch_size, disable, component_cfg, n_process)\u001b[0m\n\u001b[0;32m   1564\u001b[0m docs_with_contexts \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_doc_with_context(text, context) \u001b[38;5;28;01mfor\u001b[39;00m text, context \u001b[38;5;129;01min\u001b[39;00m texts\n\u001b[0;32m   1566\u001b[0m )\n\u001b[0;32m   1567\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipe(\n\u001b[0;32m   1568\u001b[0m     docs_with_contexts,\n\u001b[0;32m   1569\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1572\u001b[0m     component_cfg\u001b[38;5;241m=\u001b[39mcomponent_cfg,\n\u001b[0;32m   1573\u001b[0m )\n\u001b[1;32m-> 1574\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs:\n\u001b[0;32m   1575\u001b[0m     context \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39m_context\n\u001b[0;32m   1576\u001b[0m     doc\u001b[38;5;241m.\u001b[39m_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\elba_ro\\.pyenv\\pyenv-win\\versions\\3.11.2\\Lib\\site-packages\\spacy\\language.py:1618\u001b[0m, in \u001b[0;36mLanguage.pipe\u001b[1;34m(self, texts, as_tuples, batch_size, disable, component_cfg, n_process)\u001b[0m\n\u001b[0;32m   1616\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pipe \u001b[38;5;129;01min\u001b[39;00m pipes:\n\u001b[0;32m   1617\u001b[0m         docs \u001b[38;5;241m=\u001b[39m pipe(docs)\n\u001b[1;32m-> 1618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs:\n\u001b[0;32m   1619\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m doc\n",
      "File \u001b[1;32mc:\\Users\\elba_ro\\.pyenv\\pyenv-win\\versions\\3.11.2\\Lib\\site-packages\\spacy\\util.py:1715\u001b[0m, in \u001b[0;36m_pipe\u001b[1;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[0;32m   1713\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs:\n\u001b[0;32m   1714\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1715\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m doc\n\u001b[0;32m   1717\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nlpaf\\annotator\\pipe\\linguistic\\hedge.py:35\u001b[0m, in \u001b[0;36mHedgeFactory.__call__\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, doc):\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# t= Timer(\"Hedge\")\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# t.start()\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     logging\u001b[38;5;241m.\u001b[39mdisable_progress_bar()\n\u001b[1;32m---> 35\u001b[0m     results, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     df_ \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[0;32m     38\u001b[0m         {\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m         }\n\u001b[0;32m     46\u001b[0m     )\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;66;03m# df_= df_[df_[\"label\"] != \"C\"]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\elba_ro\\.pyenv\\pyenv-win\\versions\\3.11.2\\Lib\\site-packages\\simpletransformers\\ner\\ner_model.py:1612\u001b[0m, in \u001b[0;36mNERModel.predict\u001b[1;34m(self, to_predict, split_on_space)\u001b[0m\n\u001b[0;32m   1608\u001b[0m     out_label_ids \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[0;32m   1609\u001b[0m         [\u001b[38;5;28mlist\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m out_label_ids], np\u001b[38;5;241m.\u001b[39mint32\n\u001b[0;32m   1610\u001b[0m     )\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mlen\u001b[39m(out_label_ids), max_len)\n\u001b[0;32m   1611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1612\u001b[0m     eval_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_and_cache_examples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1613\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_predict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredict_examples\u001b[49m\n\u001b[0;32m   1614\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1615\u001b[0m     eval_sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(eval_dataset)\n\u001b[0;32m   1616\u001b[0m     eval_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m   1617\u001b[0m         eval_dataset, sampler\u001b[38;5;241m=\u001b[39meval_sampler, batch_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size\n\u001b[0;32m   1618\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\elba_ro\\.pyenv\\pyenv-win\\versions\\3.11.2\\Lib\\site-packages\\simpletransformers\\ner\\ner_model.py:1885\u001b[0m, in \u001b[0;36mNERModel.load_and_cache_examples\u001b[1;34m(self, data, evaluate, no_cache, to_predict)\u001b[0m\n\u001b[0;32m   1883\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1884\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Converting to features started.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1885\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_examples_to_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# XLNet has a CLS token at the end\u001b[39;49;00m\n\u001b[0;32m   1891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcls_token_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxlnet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcls_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcls_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcls_token_segment_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxlnet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1894\u001b[0m \u001b[43m        \u001b[49m\u001b[43msep_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msep_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1895\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# RoBERTa uses an extra separator b/w pairs of sentences,\u001b[39;49;00m\n\u001b[0;32m   1896\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\u001b[39;49;00m\n\u001b[0;32m   1897\u001b[0m \u001b[43m        \u001b[49m\u001b[43msep_token_extra\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mMODELS_WITH_EXTRA_SEP_TOKEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1898\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# PAD on the left for XLNet\u001b[39;49;00m\n\u001b[0;32m   1899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_on_left\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxlnet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_segment_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxlnet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1902\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_label_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_label_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprocess_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocess_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1904\u001b[0m \u001b[43m        \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1905\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiprocessing_chunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1907\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1908\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_multiprocessing_for_evaluation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_multiprocessing_for_evaluation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1909\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1911\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_cache:\n\u001b[0;32m   1912\u001b[0m         torch\u001b[38;5;241m.\u001b[39msave(features, cached_features_file)\n",
      "File \u001b[1;32mc:\\Users\\elba_ro\\.pyenv\\pyenv-win\\versions\\3.11.2\\Lib\\site-packages\\simpletransformers\\ner\\ner_utils.py:474\u001b[0m, in \u001b[0;36mconvert_examples_to_features\u001b[1;34m(examples, label_list, max_seq_length, tokenizer, cls_token_at_end, cls_token, cls_token_segment_id, sep_token, sep_token_extra, pad_on_left, pad_token, pad_token_segment_id, pad_token_label_id, sequence_a_segment_id, mask_padding_with_zero, process_count, chunksize, silent, use_multiprocessing, mode, use_multiprocessing_for_evaluation)\u001b[0m\n\u001b[0;32m    452\u001b[0m     examples \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    453\u001b[0m         (\n\u001b[0;32m    454\u001b[0m             examples[i : i \u001b[38;5;241m+\u001b[39m chunksize],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    470\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(examples), chunksize)\n\u001b[0;32m    471\u001b[0m     ]\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Pool(process_count) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[1;32m--> 474\u001b[0m         features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m    475\u001b[0m             tqdm(\n\u001b[0;32m    476\u001b[0m                 p\u001b[38;5;241m.\u001b[39mimap(\n\u001b[0;32m    477\u001b[0m                     convert_examples_with_multiprocessing,\n\u001b[0;32m    478\u001b[0m                     examples,\n\u001b[0;32m    479\u001b[0m                 ),\n\u001b[0;32m    480\u001b[0m                 total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(examples),\n\u001b[0;32m    481\u001b[0m                 disable\u001b[38;5;241m=\u001b[39msilent,\n\u001b[0;32m    482\u001b[0m             )\n\u001b[0;32m    483\u001b[0m         )\n\u001b[0;32m    485\u001b[0m         features \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    486\u001b[0m             feature \u001b[38;5;28;01mfor\u001b[39;00m feature_group \u001b[38;5;129;01min\u001b[39;00m features \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m feature_group\n\u001b[0;32m    487\u001b[0m         ]\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\elba_ro\\.pyenv\\pyenv-win\\versions\\3.11.2\\Lib\\site-packages\\tqdm\\notebook.py:249\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    248\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[1;32m--> 249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    250\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m    252\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\elba_ro\\.pyenv\\pyenv-win\\versions\\3.11.2\\Lib\\site-packages\\tqdm\\std.py:1170\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;66;03m# If the bar is disabled, then just walk the iterable\u001b[39;00m\n\u001b[0;32m   1168\u001b[0m \u001b[38;5;66;03m# (note: keep this check outside the loop for performance)\u001b[39;00m\n\u001b[0;32m   1169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable:\n\u001b[1;32m-> 1170\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1171\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\elba_ro\\.pyenv\\pyenv-win\\versions\\3.11.2\\Lib\\multiprocessing\\pool.py:861\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m--> 861\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    863\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items\u001b[38;5;241m.\u001b[39mpopleft()\n",
      "File \u001b[1;32mc:\\Users\\elba_ro\\.pyenv\\pyenv-win\\versions\\3.11.2\\Lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "experiment_eval_dict = {}\n",
    "b = True\n",
    "score = {}\n",
    "from tqdm import tqdm\n",
    "for experiment in tqdm(experiments[1:]):\n",
    "    print(experiment)\n",
    "    splits = experiment.split(\"_\")\n",
    "    ideology = splits[0]\n",
    "    model_type = splits[1]\n",
    "    \n",
    "    # if model_type == \"chatgpt\": continue\n",
    "    shot = splits[2].replace(\"shot\", \"\")\n",
    "    steered = f\"_{splits[3]}_{splits[4]}\" if len(splits) >4 else \"\"\n",
    "    experiment_eval_dict[experiment] = evaluator.Evaluator(model_type=model_type,\n",
    "                                                           ideology=ideology, \n",
    "                                                           shot_num=int(shot),\n",
    "                                                           steered=steered,\n",
    "                                                           root_path=\"\")\n",
    "    #score[ideology] = experiment_eval_dict[experiment].score_style(is_for_ineffective=True) if ideology not in score else score[ideology]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_id</th>\n",
       "      <th>mpqa_assessments</th>\n",
       "      <th>mpqa_doubt</th>\n",
       "      <th>mpqa_authority</th>\n",
       "      <th>mpqa_emphasis</th>\n",
       "      <th>mpqa_necessity</th>\n",
       "      <th>mpqa_causation</th>\n",
       "      <th>mpqa_generalization</th>\n",
       "      <th>mpqa_structure</th>\n",
       "      <th>mpqa_conditionals</th>\n",
       "      <th>...</th>\n",
       "      <th>mpqa_possibility</th>\n",
       "      <th>mpqa_wants</th>\n",
       "      <th>mpqa_contrast</th>\n",
       "      <th>mpqa_priority</th>\n",
       "      <th>mpqa_difficulty</th>\n",
       "      <th>mpqa_inyourshoes</th>\n",
       "      <th>mpqa_rhetoricalquestion</th>\n",
       "      <th>mpqa_argumentative</th>\n",
       "      <th>mpqa_token_ratio</th>\n",
       "      <th>mpqa_args_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.042</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.081</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.028</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.019</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.032</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2704</th>\n",
       "      <td>3493</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.036</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2705</th>\n",
       "      <td>3494</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.048</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2706</th>\n",
       "      <td>3495</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.009</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2707</th>\n",
       "      <td>3496</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.026</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2708</th>\n",
       "      <td>3497</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.072</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2709 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      input_id  mpqa_assessments  mpqa_doubt  mpqa_authority  mpqa_emphasis  \\\n",
       "0            0                 0           0               1              0   \n",
       "1            1                 0           0               0              0   \n",
       "2            2                 0           0               0              0   \n",
       "3            3                 0           0               0              0   \n",
       "4            4                 0           0               0              0   \n",
       "...        ...               ...         ...             ...            ...   \n",
       "2704      3493                 0           0               0              0   \n",
       "2705      3494                 1           0               0              0   \n",
       "2706      3495                 0           0               0              0   \n",
       "2707      3496                 0           0               0              0   \n",
       "2708      3497                 0           0               0              0   \n",
       "\n",
       "      mpqa_necessity  mpqa_causation  mpqa_generalization  mpqa_structure  \\\n",
       "0                  2               0                    0               0   \n",
       "1                  1               0                    0               0   \n",
       "2                  1               1                    0               0   \n",
       "3                  2               1                    0               0   \n",
       "4                  1               0                    0               0   \n",
       "...              ...             ...                  ...             ...   \n",
       "2704               1               1                    0               0   \n",
       "2705               1               1                    0               0   \n",
       "2706               2               0                    0               0   \n",
       "2707               1               0                    0               0   \n",
       "2708               2               1                    0               0   \n",
       "\n",
       "      mpqa_conditionals  ...  mpqa_possibility  mpqa_wants  mpqa_contrast  \\\n",
       "0                     0  ...                 0           0              1   \n",
       "1                     1  ...                 0           0              1   \n",
       "2                     0  ...                 1           0              1   \n",
       "3                     0  ...                 0           0              1   \n",
       "4                     0  ...                 1           0              1   \n",
       "...                 ...  ...               ...         ...            ...   \n",
       "2704                  0  ...                 1           0              1   \n",
       "2705                  0  ...                 0           0              0   \n",
       "2706                  0  ...                 0           0              0   \n",
       "2707                  0  ...                 0           0              0   \n",
       "2708                  1  ...                 1           0              1   \n",
       "\n",
       "      mpqa_priority  mpqa_difficulty  mpqa_inyourshoes  \\\n",
       "0                 3                0                 0   \n",
       "1                 1                0                 0   \n",
       "2                 1                0                 0   \n",
       "3                 1                0                 0   \n",
       "4                 1                1                 0   \n",
       "...             ...              ...               ...   \n",
       "2704              1                0                 0   \n",
       "2705              1                0                 0   \n",
       "2706              0                0                 0   \n",
       "2707              1                0                 0   \n",
       "2708              1                0                 0   \n",
       "\n",
       "      mpqa_rhetoricalquestion  mpqa_argumentative  mpqa_token_ratio  \\\n",
       "0                           0                   7             0.042   \n",
       "1                           0                   4             0.081   \n",
       "2                           0                   6             0.028   \n",
       "3                           0                   5             0.019   \n",
       "4                           0                   5             0.032   \n",
       "...                       ...                 ...               ...   \n",
       "2704                        0                   5             0.036   \n",
       "2705                        0                   7             0.048   \n",
       "2706                        0                   2             0.009   \n",
       "2707                        0                   3             0.026   \n",
       "2708                        0                   8             0.072   \n",
       "\n",
       "      mpqa_args_count  \n",
       "0                   7  \n",
       "1                   4  \n",
       "2                   6  \n",
       "3                   5  \n",
       "4                   5  \n",
       "...               ...  \n",
       "2704                5  \n",
       "2705                7  \n",
       "2706                2  \n",
       "2707                3  \n",
       "2708                8  \n",
       "\n",
       "[2709 rows x 21 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"data/llms_out/new/style_features/style_features_liberal_llamav2_0shot.parquet\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    " # Load a suitable pre-trained Sentence Transformer model\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import evaluate\n",
    "meteor = evaluate.load('meteor')\n",
    "# BAAI/bge-m3\n",
    "#stmodel = SentenceTransformer('Salesforce/SFR-Embedding-Mistral', device=\"cuda\") #SentenceTransformer('allenai/longformer-base-4096')\n",
    "def getsim(df, is_meteor: bool= False):\n",
    "  similarity = []\n",
    "  for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing rows\"):\n",
    "      # Define long texts (up to 2048 tokens or as per model’s capacity)\n",
    "      text1 =row[\"text\"]\n",
    "      text2 = row[\"effective_argument\"]\n",
    "      \n",
    "      if type(text1) == str and type(text2) == str and len(text1)>0 and len(text2)>0:\n",
    "          # Generate embeddings\n",
    "          #embeddings1 = stmodel.encode(text1, normalize_embeddings=True, convert_to_tensor=True)\n",
    "          #embeddings2 = stmodel.encode(text2, normalize_embeddings=True, convert_to_tensor=True)\n",
    "\n",
    "          # Calculate cosine similarity\n",
    "          #cosine_sim = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "          #similarity.append(cosine_sim.item())\n",
    "          #print(cosine_sim)\n",
    "\n",
    "          #embeddings_1 = bge_m3.encode(text1, normalize_embeddings=True)\n",
    "          #embeddings_2 = bge_m3.encode(text2, normalize_embeddings=True)\n",
    "          ###### sim_val = get_sfr_similarity(text1, text2)\n",
    "\n",
    "          #sim_val = embeddings_1 @ embeddings_2.T\n",
    "          # print(similarity)\n",
    "          if  is_meteor:\n",
    "\n",
    "            similarity.append(meteor.compute(predictions=[text1], references=[text2])[\"meteor\"])\n",
    "          else:\n",
    "            similarity.append(get_bge_m3_sim(text1, text2))#.item())\n",
    "\n",
    "      else:\n",
    "          print(f\"type {text1}: { type(text1) }\")\n",
    "          print(f\"type {text2}: { type(text2) }\")\n",
    "  return similarity\n",
    "        # Print cosine similarity\n",
    "\n",
    "\n",
    "\n",
    "def get_perc(lst_, threshold:float = 0.5):\n",
    "   \n",
    "    x = [1 if y>=threshold else 0 for y in lst_ ]\n",
    "    percentage_of_ones = (sum(x) / len(x)) * 100\n",
    "\n",
    "    return percentage_of_ones\n",
    " \n",
    "def get_similar_perc(sims_all, k, threshold:float = 0.7):\n",
    "    def get_(a):\n",
    "        if a>threshold:\n",
    "            return 1\n",
    "        else: return 0\n",
    "\n",
    "    x = [get_(y) for y in sims_all[k] ]\n",
    "    percentage_of_ones = (sum(x) / len(x)) * 100\n",
    "\n",
    "    return percentage_of_ones\n",
    "\n",
    "sims_all_mistral = {}\n",
    "ideologies = [\"liberal\", \"conservative\"]\n",
    "for ideo_ in ideologies:\n",
    "    print(f\"\\n\\n******** {ideo_.capitalize()} ************** \")\n",
    "\n",
    "    for sett in [\"0shot\", \"1shot\", \"0shot_steered_meanl0.2\",  \"0shot_steered_meanl0.5\", \n",
    "                  \"0shot_steered_medianl0.2\",  \"0shot_steered_medianl0.5\"]:\n",
    "        print(\"\\n-----------------------\")\n",
    "        models = [\"chatgpt\", \"llamav2\"] if sett.find(\"steered\") <0 else [ \"llamav2\"]\n",
    "        sign_dict = get_siginficant_scores()\n",
    "        for model in  models:\n",
    "            print(\"\\n\")\n",
    "            key_ = f\"{ideo_}_{model}_{sett}\"\n",
    "            df = experiment_eval_dict[key_].successful_df\n",
    "            original_score = experiment_eval_dict[key_].effectiveness_original_scores\n",
    "            ori_mean= np.median(original_score['effective'].values.tolist())\n",
    "            \n",
    "            print(f\"ORIGINAL {round(ori_mean, 2)}\")\n",
    "            for prompt, df_ in df.groupby(\"prompt\"):\n",
    "                #if prompt in [\"base\", \"content\", \"ideology\", \"style\", \"content_style_ideology\"]:\n",
    "                df_ = df_[~df_[\"effective\"].isna()]\n",
    "                effective_vals = df_.effective.values.tolist()\n",
    "                experiment_ = f\"{key_}-{prompt}\"\n",
    "                if prompt in sign_dict[key_]: #ori_mean<np.median(effective_vals):\n",
    "                    print(f\"{experiment_}: {round(get_perc(effective_vals, 0.5) , 2)}\\% \"\n",
    "                            f\"\\t Median {round(np.median(effective_vals), 2)} \"\n",
    "                            f\"(+-{round(np.array(effective_vals).std(), 2)}) \\t\"\n",
    "                            f\"- Mean {round(np.array(effective_vals).mean(), 2)}\"\n",
    "                            f\"- GREATER THAN ORI {ori_mean<np.median(effective_vals)}\"\n",
    "                            f\"IS SIGNIFICANT: {prompt in sign_dict[key_]}\")\n",
    "                  \n",
    "                  #if experiment_ not in sims_all_mistral.keys():\n",
    "                  #    sims_all_mistral[experiment_] = getsim(df_, True)# compute_tfidf_cosine_similarity(df_)\n",
    "                  #print(f\"{experiment_}: {round(get_similar_perc(sims_all_mistral, experiment_, 0.5), 2)}\\% \"\n",
    "                  #      f\"\\t Median {round(np.median(np.array(sims_all_mistral[experiment_])), 2)} \"\n",
    "                  #      f\"(+-{round(np.array(sims_all_mistral[experiment_]).std(), 2)}) \\t\"\n",
    "                  #      f\"- Mean {round(np.array(sims_all_mistral[experiment_]).mean(), 2)}\")\n",
    "\n",
    "original_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score[\"liberal\"][\"features_score\"].mean())\n",
    "print(score[\"liberal\"][\"features_score\"].std())\n",
    "\n",
    "print(score[\"conservative\"][\"features_score\"].mean())\n",
    "print(score[\"conservative\"][\"features_score\"].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed = 0.0\n",
    "total = 0\n",
    "for key, evaluator_exp in experiment_eval_dict.items():\n",
    "    if evaluator_exp.model_type == \"llamav2\":\n",
    "        failed = failed + evaluator_exp.failed_num\n",
    "        total = total + evaluator_exp.total\n",
    "\n",
    "round(failed*100/total,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_eval_dict.keys()\n",
    "experiment_eval_dict[\"conservative_llamav2_1shot\"].eff_score_sign\n",
    "experiment_eval_dict[\"conservative_llamav2_1shot\"].failed_num\n",
    "experiment_eval_dict[\"conservative_llamav2_1shot\"].successful_df[\"effective\"].value_counts()\n",
    "experiment_eval_dict[\"conservative_llamav2_1shot\"].style_score_sign\n",
    "experiment_eval_dict[\"conservative_llamav2_1shot\"].eff_score_sign\n",
    "#experiment_eval_dict[\"conservative_llamav2_1shot\"].successful_df[\"effective\"].value_counts()\n",
    "def get_siginficant_scores():\n",
    "    promt_signi = {}\n",
    "    for k,v in experiment_eval_dict.items():\n",
    "        eff_score_df = v.eff_score_sign\n",
    "        eff_score_df = eff_score_df[eff_score_df[\"p_value\"] <  0.05 ]#\n",
    "        #print(f\"\\n {k}: {', '.join(eff_score_df['prompt'].values.tolist())}\")\n",
    "        promt_signi[k] = eff_score_df['prompt'].values.tolist()\n",
    "    return promt_signi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(\"data/llms_out/new/style_features/style_features_conservative_llamav2_0shot.parquet\").describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_all = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_eval_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "ideologies = [\"liberal\", \"conservative\"]\n",
    "for ideo_ in ideologies:\n",
    "    print(f\"\\n\\n******** {ideo_.capitalize()} ************** \")\n",
    "\n",
    "    for sett in [\"0shot\", \"1shot\", \"0shot_steered_meanl0.2\",  \"0shot_steered_meanl0.5\"]:\n",
    "        print(\"\\n-----------------------\")\n",
    "        models = [\"chatgpt\", \"llamav2\"] if sett.find(\"steered\") <0 else [ \"llamav2\"] \n",
    "        for model in  models:\n",
    "            print(\"\\n\")\n",
    "            key_ = f\"{ideo_}_{model}_{sett}\"\n",
    "            df = experiment_eval_dict[key_].successful_df\n",
    "            for prompt, df_ in df.groupby(\"prompt\"):\n",
    "                \n",
    "                if prompt in [\"base\", \"content\", \"ideology\", \"style\", \"content_style_ideology\"]:\n",
    "                 \n",
    "                    experiment_ = f\"{key_}-{prompt}\"\n",
    "                    if experiment_ not in sims_all.keys():\n",
    "                        sims_all[experiment_] = getsim(df_)# compute_tfidf_cosine_similarity(df_)\n",
    "                    print(f\"{experiment_}: {round(get_similar_perc(sims_all, experiment_, 0.5), 2)}\\% \"\n",
    "                          f\"\\t Median {round(np.median(np.array(sims_all[experiment_])), 2)} \"\n",
    "                          f\"(+-{round(np.array(sims_all[experiment_]).std(), 2)}) \\t\"\n",
    "                          f\"- Mean {round(np.array(sims_all[experiment_]).mean(), 2)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def get_similar_perc(sims_all, k, threshold:float = 0.7):\n",
    "    def get_(a):\n",
    "        if a>threshold:\n",
    "            return 1\n",
    "        else: return 0\n",
    "\n",
    "    x = [get_(y) for y in sims_all[k] ]\n",
    "    percentage_of_ones = (sum(x) / len(x)) * 100\n",
    "    \n",
    "    return percentage_of_ones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_tfidf_cosine_similarity(df):\n",
    "    cosine_sim= []\n",
    "    df.fillna(\"\", inplace=True)\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing rows\"):\n",
    "        # Define long texts (up to 2048 tokens or as per model’s capacity)\n",
    "        text1 =row[\"text\"]\n",
    "        text2 = row[\"effective_argument\"]\n",
    "        if len(text1)>0 and len(text2)>0:\n",
    "            # Initialize the TF-IDF vectorizer\n",
    "            tfidf_vectorizer = TfidfVectorizer()\n",
    "            \n",
    "            # Compute the TF-IDF vectors for each text\n",
    "            tfidf_matrix = tfidf_vectorizer.fit_transform([text1, text2])\n",
    "            \n",
    "            # Calculate cosine similarity between the TF-IDF vectors\n",
    "            cosine_sim.append(cosine_similarity(tfidf_matrix)[0][1])\n",
    "    return cosine_sim\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Load a suitable pre-trained Sentence Transformer model\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "stmodel = SentenceTransformer('BAAI/bge-large-en-v1.5') #SentenceTransformer('allenai/longformer-base-4096')\n",
    "def getsim(df):\n",
    "    similarity = []\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing rows\"):\n",
    "        # Define long texts (up to 2048 tokens or as per model’s capacity)\n",
    "        text1 =row[\"text\"]\n",
    "        text2 = row[\"effective_argument\"]\n",
    "        if type(text1) == str and type(text2) == str and len(text1)>0 and len(text2)>0:\n",
    "            # Generate embeddings\n",
    "            #embeddings1 = stmodel.encode(text1, normalize_embeddings=True, convert_to_tensor=True)\n",
    "            #embeddings2 = stmodel.encode(text2, normalize_embeddings=True, convert_to_tensor=True)\n",
    "\n",
    "            # Calculate cosine similarity\n",
    "            #cosine_sim = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "            #similarity.append(cosine_sim.item())\n",
    "            #print(cosine_sim)\n",
    "\n",
    "            embeddings_1 = stmodel.encode(text1, normalize_embeddings=True)\n",
    "            embeddings_2 = stmodel.encode(text2, normalize_embeddings=True)\n",
    "            sim_val = embeddings_1 @ embeddings_2.T\n",
    "            #embeddings_1 = stmodel.encode(sentences_1, normalize_embeddings=True)\n",
    "            #embeddings_2 = model.encode(sentences_2, normalize_embeddings=True)\n",
    "            #similarity = embeddings_1 @ embeddings_2.T\n",
    "            # print(similarity)\n",
    "            similarity.append(sim_val.item())\n",
    "        else:\n",
    "            print(f\"type {text1}: { type(text1) }\")\n",
    "            print(f\"type {text2}: { type(text2) }\")\n",
    "    return similarity\n",
    "        # Print cosine similarity\n",
    "\n",
    "\n",
    "\n",
    "# https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/baai_general_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effec_count.sort_values([\"ratio\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ps= ['base', 'content', 'content_style', 'content_style_ideology',\n",
    "       'style', 'style_ideology', 'ideology']\n",
    "def get_vals(eval_obj):\n",
    "    style_score_df = eval_obj.score_style()\n",
    "    merged = pd.merge(\n",
    "            eval_obj.merged_df,\n",
    "            style_score_df,\n",
    "            left_index = True,\n",
    "            right_index=True,\n",
    "            how=\"inner\",\n",
    "            \n",
    "        )\n",
    "    merged = merged[merged[\"success\"]]\n",
    "\n",
    "    def _apply(row):\n",
    "        row[\"effective_str\"] = row[\"effective\"] >=0.5\n",
    "        return row\n",
    "    df_ =eval_obj.successful_df.apply(_apply, axis=1)\n",
    "    df_ = df_[~df_[\"effective\"].isna()]\n",
    "    stye_scores_prompt =  merged[[\"prompt\", \"features_score\"]].groupby([\"prompt\"])[\"features_score\"].agg([ \"mean\", \"std\"]).round(2)\n",
    "    effec_count =  pd.crosstab(df_[\"prompt\"], df_[\"effective_str\"]) \n",
    "\n",
    "    effec_count[\"ratio\"] = round(effec_count[True]/(effec_count[False]+effec_count[True])*100)\n",
    "    effec_count = effec_count.sort_values([\"ratio\"], ascending=True)\n",
    "    #effec_count.set_index(\"prompt\", inplace=True)\n",
    "    eff_p_prompt = eval_obj.successful_df[[\"prompt\", \"effective\"]].groupby([\"prompt\"])[\"effective\"].agg([ \"mean\", \"std\"]).round(2)\n",
    "    return stye_scores_prompt, eff_p_prompt, effec_count\n",
    "\n",
    "res_key = {}\n",
    "for key, evaluator_exp in experiment_eval_dict.items():\n",
    "    key_ = key.replace(\"liberal_\", \"\").replace(\"conservative_\", \"\")\n",
    "    print(key)\n",
    "    ideology = evaluator_exp.ideology\n",
    "    model_type = evaluator_exp.model_type\n",
    "    #if (ideology == \"liberal\" and key.find(\"steered\") <0 ) or ideology==\"conservative\":\n",
    "\n",
    "    stye_scores_prompt, eff_p_prompt, effec_count = get_vals(evaluator_exp)\n",
    "    #print(key +\"++++++++++++++++++++++\")\n",
    "    #print(evaluator_exp.eff_score_sign)\n",
    "    # print(effec_count)\n",
    "    \n",
    "    res_key[key_] = {} if key_ not in res_key.keys() else res_key[key_]\n",
    "    re_str = res_key[key_]\n",
    "    for p in ps:\n",
    "        style_score = f'{stye_scores_prompt.loc[p][\"mean\"]} ($\\pm {round(stye_scores_prompt.loc[p][\"std\"],1)}$)'\n",
    "        eff_score = f'{round(effec_count.loc[p][\"ratio\"]*100)}\\%'\n",
    "        if p not in re_str.keys(): re_str[p]= \"\"\n",
    "\n",
    "        if ideology == \"conservative\":\n",
    "            re_str[p]=  f\"& {p} & {eff_score} & {style_score} & X.XX &&\" + re_str[p]\n",
    "        else: \n",
    "            re_str[p]=  re_str[p] + f\" {eff_score} & {style_score} & X.XX  \\\\\\ \\n\" \n",
    "    res_key[key_] = re_str\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, evaluator_exp in experiment_eval_dict.items():\n",
    "    key_ = key.replace(\"liberal_\", \"\").replace(\"conservative_\", \"\")\n",
    "\n",
    "\n",
    "    #f = open(f\"data/llms_out/new/eval/{key_}.txt\", \"w\")\n",
    "    dict_ = res_key[key_]\n",
    "    x = \" \".join([ v for _,v in dict_.items()])\n",
    "    print(f\"{key_}  {x}\")\n",
    "    #f.write(f\"{key_}  {x}\")\n",
    "    #f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scores[\"liberal_chatgpt_1shot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iesta.data.feature_score import get_top_features\n",
    "\n",
    "style_score_df[get_top_features(\"conservative\")[\"feature\"].values.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "corr_prompt_dict = {\"liberal\": [], \"conservative\": []}\n",
    "for key, evaluator_exp in experiment_eval_dict.items():\n",
    "    ideology = evaluator_exp.ideology\n",
    "    model_type = evaluator_exp.model_type\n",
    "\n",
    "    style_score_df = evaluator_exp.score_style()\n",
    "    \n",
    "    corr_df = evaluator_exp.calc_corr_two_binary()#evaluator_exp.corr_toxicity_no_response\n",
    "    corr_df =corr_df[corr_df[\"group_type\"] == \"\"]\n",
    "    corr_df = corr_df.drop('group_value', axis=1)\n",
    "    corr_df = corr_df.drop('group_type', axis=1)\n",
    "    def remove_ide(row):\n",
    "        row['experiment'] = row['experiment'].replace(f\"{ideology}_\", \"\")\n",
    "        return row\n",
    "    corr_df = corr_df.apply(remove_ide, axis=1)\n",
    "    corr_df = corr_df.set_index(\"experiment\")\n",
    "    corr_df = corr_df.add_suffix(f'_{ideology}')\n",
    "\n",
    "    corr_prompt_dict[ideology].append(corr_df)\n",
    "\n",
    "#liberal = pd.concat(corr_prompt_dict[\"liberal\"], join='outer')\n",
    "#conservative = pd.concat(corr_prompt_dict[\"conservative\"])\n",
    "#all_corr_df = pd.concat([conservative, liberal], axis=1)\n",
    "#all_corr_df[\"correlation_conservative\"] = round(all_corr_df[\"correlation_conservative\"], 2)\n",
    "style_score_df[\"features_score\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_extended_counts(experiment_eval_dict, count_type):\n",
    "\n",
    "    count_prompt_dict = {\"liberal\": [], \"conservative\": []}\n",
    "    for key, evaluator_exp in experiment_eval_dict.items():\n",
    "        ideology = evaluator_exp.ideology\n",
    "        model_type = evaluator_exp.model_type\n",
    "        if model_type !=\"llamav2\":\n",
    "            continue\n",
    "        if count_type == \"toxicity\":\n",
    "            count_df = evaluator_exp.count_success_per_toxic_df\n",
    "        elif count_type == \"prompt\":\n",
    "            count_df = evaluator_exp.count_success_per_prompt_df\n",
    "        else:\n",
    "            count_df = evaluator_exp.count_success_per_ideology_prompt_df\n",
    "        count_prompt_df = count_df[[\"fail_perc\"]].rename(columns= {\"fail_perc\": key.replace(f\"{ideology}_\", \"\")}).T\n",
    "        count_prompt_df = count_prompt_df.drop('All', axis=1)\n",
    "        count_prompt_df = count_prompt_df.add_suffix(f'_{ideology}')\n",
    "        count_prompt_df.score_style()\n",
    "        count_prompt_dict[ideology].append(count_prompt_df)\n",
    "        #break\n",
    "\n",
    "    liberal = pd.concat(count_prompt_dict[\"liberal\"])\n",
    "    conservative = pd.concat(count_prompt_dict[\"conservative\"])\n",
    "    all_prompt_extended_df = pd.concat([conservative, liberal], axis=1)\n",
    "    return all_prompt_extended_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideology_prompt_count = get_extended_counts(experiment_eval_dict, \"ideology_prompt\")\n",
    "ideology_prompt_count.to_csv(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_prompt_count = get_extended_counts(experiment_eval_dict, \"toxicity\")#.to_csv(\"data/out/toxicity_success_extended.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_extended_df = pd.concat([toxicity_prompt_count, ideology_prompt_count], axis=1)\n",
    "count_extended_df[['Toxic_conservative', 'Not Toxic_conservative', 'True_conservative', 'False_conservative',\n",
    "                   'Toxic_liberal', 'Not Toxic_liberal', 'True_liberal', 'False_liberal']].to_csv(\"data/out/failed_to_respond_perc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"data/liberal_longformer/liberal_longformer/epoch_6/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " tokenizer = AutoTokenizer.from_pretrained(\n",
    "            f\"{self.model_org}\" f\"/{self.output_dir}\"\n",
    "        )\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"data/liberal_longformer/liberal_longformer/epoch_6/\"\n",
    ")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import ntpath\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "import html\n",
    "\n",
    "out = \"data/llms_out/new/processed/*_processed.csv\"\n",
    "import json\n",
    "\n",
    "def path_leaf(path):\n",
    "    head, tail = ntpath.split(path)\n",
    "    return tail or ntpath.basename(head)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_cross_model_format(experiment_file_path: str ):\n",
    "    reformatted_path = \"data/llms_out/new/cross_models_format\"\n",
    "    experiment_name = path_leaf(experiment_file_path).replace('processed.csv', '')\n",
    "    path = f\"{reformatted_path}/{experiment_name}.jsonl\"\n",
    "    if Path(path).is_file():\n",
    "        print(\"File found\")\n",
    "        return pd.read_json(path_or_buf=path, lines=True)\n",
    "        \n",
    "\n",
    "    df_ = pd.read_csv(experiment_file_path)\n",
    "    print(\"Original Length: \", len(df_))\n",
    "    df_ = df_[df_['success']]\n",
    "    print(\"Successful Length: \", len(df_))\n",
    "\n",
    "    df_ = df_[[ 'idx', 'prompt', 'ineffective_argument', 'effective_argument']]\n",
    "    \n",
    "    #df_['prompt'] = df_['prompt']+\"-\"+experiment_name\n",
    "\n",
    "    df_by_models = []\n",
    "\n",
    "    for model, sub in df_.groupby(\"prompt\"):\n",
    "        print(model, \": \", len(sub))\n",
    "        sub = sub.rename(columns={'effective_argument': model, 'ineffective_argument': 'reference'})\n",
    "        sub = sub[[\"idx\", model, 'reference']]\n",
    "  \n",
    "        df_by_models.append(sub)\n",
    "        \n",
    "    merged_df = reduce(lambda left, right: pd.merge(left, right, on=['idx', 'reference'], how='outer'), df_by_models)\n",
    "    print(\"merged: \", len(merged_df))\n",
    "    merged_df.set_index(\"idx\", inplace=True)\n",
    "    \n",
    "    merged_df.fillna(\"\", inplace=True)\n",
    "    for _, row in merged_df.iterrows():\n",
    "        with open(path, 'a') as file:\n",
    "            row_dict = row.to_dict()\n",
    "            for k in row_dict.keys():\n",
    "                row_dict[k] = row_dict[k].replace(\"<url>\", \"URL\").replace(\"<URL>\", \"URL\").replace(\"<\", \" \").replace(\">\", \" \") \n",
    "            file.write(json.dumps(row_dict)+\"\\n\")\n",
    "\n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import html\n",
    "\n",
    "# Sample data with HTML content\n",
    "data = {\n",
    "    \"title\": \"Example\",\n",
    "    \"html_content\": \"<url>\"\n",
    "}\n",
    "\n",
    "# Escape HTML content\n",
    "data['html_content'] = html.escape(data['html_content'])\n",
    "\n",
    "# Convert to JSON string\n",
    "json_string = json.dumps(data)\n",
    "\n",
    "# Write to a JSONL file\n",
    "with open('output.jsonl', 'w') as file:\n",
    "    file.write(json_string + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_json(\"output.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for experiment_path in glob.glob(out):\n",
    "    print(experiment_path)\n",
    "    get_cross_model_format(experiment_path)\n",
    "    print()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from tqdm import tqdm\n",
    "# arg1 ORIGINAL\n",
    "# arg2 LLAMA\n",
    "# arg3 CHATGPT\n",
    "def get_humn_eval_format( ):\n",
    "    best_models_path: str = \"data/llms_out/new/processed/best/*.csv\" \n",
    "\n",
    "    best_prompt_dict = {\"conservative_chatgpt_0shot\": \"ideology\",\n",
    "                        \"conservative_llamav2_0shot_steered_meanl0.2\": \"style\",\n",
    "                        \"liberal_chatgpt_0shot\": \"content_style_ideology\",\n",
    "                        \"liberal_llamav2_1shot\": \"style\"}\n",
    "    dfs = {\"liberal\":[], \"conservative\": []}\n",
    "    for exp in tqdm(glob.glob(best_models_path)):\n",
    "        \n",
    "        experiment_name = path_leaf(exp).replace('_processed.csv', '')\n",
    "        df_ = pd.read_csv(exp)\n",
    "        print(\"Original Length: \", len(df_))\n",
    "        df_ = df_[df_['success']]\n",
    "        print(\"Successful Length: \", len(df_))\n",
    "        print(f\"Filtering by prompt {best_prompt_dict[experiment_name]}\")\n",
    "        df_ = df_[df_['prompt'] == best_prompt_dict[experiment_name]]\n",
    "        print(\"After prompt filer Length: \", len(df_))\n",
    "\n",
    "        df_ = df_[[ 'idx', 'ineffective_argument', 'effective_argument']]\n",
    "\n",
    "        #df_['prompt'] = df_['prompt']+\"-\"+experiment_name\n",
    "\n",
    "        df_by_models = []\n",
    "        arg_tag = \"arg2\" if experiment_name.find(\"llamav2\")>-1 else  \"arg3\"\n",
    "        assert (arg_tag == \"arg2\" and experiment_name.find(\"llamav2\")>-1  ) or (arg_tag == \"arg3\" and experiment_name.find(\"chatgpt\")>-1  )\n",
    "        df_ = df_.rename(columns={'effective_argument': arg_tag, 'ineffective_argument': 'arg1'})\n",
    "        df_ = df_[[\"idx\", \"arg1\", arg_tag]]\n",
    "        ideology = experiment_name.split(\"_\")[0]\n",
    "        df_[\"batch\"] = ideology\n",
    "        df_.set_index(['idx', 'arg1', 'batch'], inplace=True)\n",
    "        dfs[ideology].append(df_)\n",
    "    \n",
    "    liberal = reduce(lambda left, right: pd.merge(left, right, on=['idx', 'arg1', 'batch'], how='inner'), dfs[\"liberal\"])\n",
    "    liberal = liberal.sample(frac=1)[:50]\n",
    "    conservative = reduce(lambda left, right: pd.merge(left, right, on=['idx', 'arg1', 'batch'], how='inner'), dfs[\"conservative\"])\n",
    "    conservative = conservative.sample(frac=1)[:50]\n",
    "    #merged_df = reduce(lambda left, right: pd.merge(left, right, on=['idx', 'arg1', \"batch\", \"arg2\", \"arg3\"], how='outer'), dfs)\n",
    "    #print(\"merged: \", len(merged_df))\n",
    "    #merged_df.set_index(\"idx\", inplace=True)\n",
    "\n",
    "    res = pd.concat([liberal, conservative]).reset_index().reset_index().rename(columns={\"index\": \"id\"})[[ \"id\", \"arg1\", \"arg2\", \"arg3\", \"batch\"]]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_humn_eval_format()\n",
    "res.to_csv(\"data/human_eval_dataset.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
