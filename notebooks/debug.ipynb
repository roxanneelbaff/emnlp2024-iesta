{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "\n",
    "    pipeline,\n",
    ")\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dotenv was True\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = LlamaTokenizer.from_pretrained(\"chavinlo/alpaca-native\")\n",
    "found = load_dotenv(find_dotenv())\n",
    "print(f\"dotenv was {found}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "ALPACA_WEIGHTS_FOLDER = \"/localdata1/EmEx/model_weights/alpaca_7b\"\n",
    "\n",
    "alpaca_model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    ALPACA_WEIGHTS_FOLDER\n",
    ").to(device)\n",
    "alpaca_tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    ALPACA_WEIGHTS_FOLDER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=alpaca_model,\n",
    "    tokenizer=alpaca_tokenizer,\n",
    "    max_length=1024,\n",
    "    # temperature=0.2,\n",
    "    # top_p=0.95,\n",
    "    # repetition_penalty=1.2\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAIChat\n",
    "from langchain import LLMChain\n",
    "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "from ast import literal_eval\n",
    "import random\n",
    "import re\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "basic_str = \"Transform the following argument to an effective argument by maintaining the original length\"\n",
    "ideology_str = \"for readers with a {ideology} political ideology\"\n",
    "content_str = \"by preserving the content of the argument\"\n",
    "style_str = \"by only changing the style of the text\"\n",
    "\n",
    "prompt_dict = {\n",
    "    \"basic\": f\"{basic_str}:\",\n",
    "    \"ideology\": f\"{basic_str} {ideology_str}:\",\n",
    "    \"content\": f\"{basic_str} {content_str}:\",\n",
    "    \"style\": f\"{basic_str} {style_str}:\",\n",
    "    \"ideology-content\": f\"{basic_str} {ideology_str} {content_str}:\",\n",
    "    \"ideology-style\": f\"{basic_str} {ideology_str} {style_str}:\",\n",
    "    \"all\": f\"{basic_str} {ideology_str} {content_str} and {style_str}:\",\n",
    "}\n",
    "\n",
    "\n",
    "def create_prompt_template(prompt):\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(prompt)\n",
    "    human_template = \"{ineffective_argument}\"\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
    "        human_template\n",
    "    )\n",
    "    return [system_message_prompt, human_message_prompt]\n",
    "\n",
    "\n",
    "liberal_chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    create_prompt_template(prompt_dict[\"all\"].format(ideology=\"liberal\"))\n",
    ")\n",
    "llm_chain = LLMChain(llm=local_llm, prompt=liberal_chat_prompt)\n",
    "result = llm_chain.run(\n",
    "    ineffective_argument=\"If there was no Kryptonite, can Superman defeat the Silver Surfer?\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nalpaca_llm = AutoModelForCausalLM.from_pretrained(\\n    ALPACA_WEIGHTS_FOLDER,\\n    # load_in_8bit=True,\\n    device_map=\"auto\",\\n)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "ALPACA_WEIGHTS_FOLDER = \"/localdata1/EmEx/model_weights/alpaca_7b\"\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(ALPACA_WEIGHTS_FOLDER)\n",
    "'''\n",
    "alpaca_llm = AutoModelForCausalLM.from_pretrained(\n",
    "    ALPACA_WEIGHTS_FOLDER,\n",
    "    # load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=alpaca_llm,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=1024,\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2,\n",
    ")\n",
    "local_model = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "template = \"\"\"Create an argument for Conservative readers related to a specific topic. Return at least 200 words.\n",
    "\n",
    "Topic:\n",
    "{topic}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"topic\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=local_model)\n",
    "qn = \"climate change\"\n",
    "print(llm_chain.run(qn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q install -i https://test.pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install cudatoolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall -y bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q install -U bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q uninstall -y bitsandbytes-cuda114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q uninstall -y bitsandbytes-cuda117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show iesta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dotenv was True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elba_ro/repos/github/conf22-style-transfer/.venv/lib/python3.9/site-packages/numba/core/decorators.py:262: NumbaDeprecationWarning: numba.generated_jit is deprecated. Please see the documentation at: https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-generated-jit for more information and advice on a suitable replacement.\n",
      "  warnings.warn(msg, NumbaDeprecationWarning)\n",
      "/home/elba_ro/repos/github/conf22-style-transfer/.venv/lib/python3.9/site-packages/visions/backends/shared/nan_handling.py:51: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def hasna(x: np.ndarray) -> bool:\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from iesta.data.huggingface_loader import IESTAHuggingFace\n",
    "from ydata_profiling import ProfileReport\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from os.path import exists\n",
    "import dataclasses\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from datasets.combine import concatenate_datasets\n",
    "from iesta.data.huggingface_loader import IESTAHuggingFace\n",
    "from ydata_profiling import ProfileReport\n",
    "from langdetect import detect\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM \n",
    "from typing import ClassVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import transformers\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Generator:\n",
    "    ideology: str  # liberal or conservative\n",
    "    model_name: str  # gpt or alpaca\n",
    "\n",
    "    data_limit: int = 500\n",
    "    data_profiling: bool = False\n",
    "    data_save: bool = False\n",
    "    seed: int = 2062021\n",
    "\n",
    "    out_file: str = \"data/llms_out/\"\n",
    "\n",
    "    use_fewshots: bool = False\n",
    "    fewshots_num_examples: int = 1  # we use 1 to 3\n",
    "    fewshots_w_semantic_similarity: bool = False\n",
    "    verbose: int = 0\n",
    "    trainingdata_profiling: bool = True\n",
    "    _MODEL_CHATGPT_: ClassVar = \"gpt-3.5-turbo\"\n",
    "    _MODEL_ALPACA_: ClassVar = \"alpaca\"\n",
    "    _LIMIT_: ClassVar = 500\n",
    "\n",
    "    # HELPERS   #\n",
    "    # --------- #\n",
    "    @staticmethod\n",
    "    def init_prompts():\n",
    "        basic_str = \"Transform the following argument to an effective argument by maintaining the original length\"\n",
    "        ideology_str = \"for readers with a {ideology} political ideology\"\n",
    "        content_str = \"by preserving the content of the argument\"\n",
    "        style_str = \"by only changing the style of the text\"\n",
    "\n",
    "        prompt_dict = {\n",
    "            \"basic\": f\"{basic_str}:\",\n",
    "            \"ideology\": f\"{basic_str} {ideology_str}:\",\n",
    "            \"content\": f\"{basic_str}, and {content_str}:\",\n",
    "            \"style\": f\"{basic_str}, and {style_str}:\",\n",
    "            \"ideology-content\": f\"{basic_str} {ideology_str} {content_str}:\",\n",
    "            \"ideology-style\": f\"{basic_str} {ideology_str} {style_str}:\",\n",
    "            \"all\": f\"{basic_str} {ideology_str} {content_str}, and {style_str}:\",\n",
    "        }\n",
    "        return prompt_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def create_prompt_template(prompt):\n",
    "        system_message_prompt = SystemMessagePromptTemplate.from_template(\n",
    "            prompt\n",
    "        )\n",
    "        human_template = \"Argument: {ineffective_argument}\"\n",
    "        human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
    "            human_template\n",
    "        )\n",
    "\n",
    "        return [system_message_prompt, human_message_prompt]\n",
    "\n",
    "    # ---- End Helpers --- #\n",
    "\n",
    "    def __post_init__(self):\n",
    "        found = load_dotenv(\"/home/elba_ro/repos/github/conf22-style-transfer/.env\")\n",
    "        print(f\"dotenv was found: {found}\")\n",
    "\n",
    "        print(\"Initializing all prompt templates in variable prompt_dict..\")\n",
    "        self.prompt_dict = Generator.init_prompts()\n",
    "\n",
    "        print(\n",
    "            f\"Initializing LLM for {self.model_name} in variable local_llm...\"\n",
    "        )\n",
    "        self.local_llm = self.get_model()\n",
    "\n",
    "        print(\n",
    "            f\"Getting filtered dataset top {self.data_limit} in variable\"\n",
    "            \"filtered_dataset...\"\n",
    "        )\n",
    "        self.filtered_dataset = self.get_data(effect=\"ineffective\")\n",
    "\n",
    "        if self.use_fewshots:\n",
    "            self.examples = self.get_examples()\n",
    "\n",
    "    def get_model(self):\n",
    "        if self.model_name == Generator._MODEL_CHATGPT_:\n",
    "            local_llm = ChatOpenAI(\n",
    "                model_name=Generator._MODEL_CHATGPT_, temperature=0\n",
    "            )\n",
    "        elif self.model_name == Generator._MODEL_ALPACA_:\n",
    "            # device = torch.device('cuda:0')\n",
    "            ALPACA_WEIGHTS_FOLDER = \"/localdata1/EmEx/model_weights/alpaca_7b\"\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained(ALPACA_WEIGHTS_FOLDER)\n",
    "            alpaca_llm = AutoModelForCausalLM.from_pretrained(\n",
    "                ALPACA_WEIGHTS_FOLDER,\n",
    "                # load_in_8bit=True,\n",
    "                device_map=\"cuda:1\",\n",
    "            )\n",
    "\n",
    "            pipe = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=alpaca_llm,\n",
    "                tokenizer=tokenizer,\n",
    "                max_length=4096,\n",
    "                temperature=0,\n",
    "                top_p=0.95,\n",
    "                repetition_penalty=1.2,\n",
    "            )\n",
    "            local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "        return local_llm\n",
    "\n",
    "    def get_data(self, effect=\"ineffective\"):\n",
    "        limit = Generator._LIMIT_\n",
    "        seed = 2062021\n",
    "        name: str = f\"notaphoenix/debateorg_w_effect_for_{self.ideology}\"\n",
    "        dataset: Dataset = load_dataset(name, split=\"test\")\n",
    "        dataset = dataset.filter(\n",
    "            lambda x: x[\"label\"] == IESTAHuggingFace._LABEL2ID_[effect]\n",
    "        ).shuffle(seed=seed)\n",
    "\n",
    "        if len(dataset) > limit:\n",
    "            dataset = dataset.select(range(limit))\n",
    "\n",
    "        print(f\"{len(dataset)} before len filter\")\n",
    "        dataset = dataset.filter(\n",
    "            lambda x: len(x[\"text\"].split(\" \")) > 10\n",
    "            and len(x[\"text\"].split(\" \")) <= 1024\n",
    "            and x[\"idx\"] != 64707\n",
    "            and detect(x[\"text\"]) == \"en\"\n",
    "        )\n",
    "        print(f\"{len(dataset)} after len filter\")\n",
    "\n",
    "        while len(dataset) < limit:\n",
    "            idxes = dataset.to_pandas()[\"idx\"].values.tolist()\n",
    "            dataset_extra: Dataset = load_dataset(name, split=\"test\")\n",
    "            dataset_extra = dataset_extra.filter(\n",
    "                lambda x: len(x[\"text\"].split(\" \")) > 10\n",
    "                and len(x[\"text\"].split(\" \")) <= 1024\n",
    "                and x[\"idx\"] != 64707\n",
    "                and detect(x[\"text\"]) == \"en\"\n",
    "            )\n",
    "\n",
    "            dataset_extra = dataset_extra.filter(\n",
    "                lambda x: x[\"label\"] == IESTAHuggingFace._LABEL2ID_[effect]\n",
    "                and [\"idx\"] not in idxes\n",
    "            ).shuffle(seed=seed)\n",
    "            dataset_extra = dataset_extra.select(range(limit - len(dataset)))\n",
    "            print(f\"{len(dataset_extra)} of extra\")\n",
    "            dataset = concatenate_datasets([dataset, dataset_extra])\n",
    "\n",
    "            print(f\"{len(dataset)} new length\")\n",
    "        print(f\"Return dataset {name} with {len(dataset)} \")\n",
    "        # dataset = dataset.map(lambda example, idx: {\"id\": idx, **example}, with_indices=True)\n",
    "\n",
    "        df = dataset.to_pandas().copy()\n",
    "        if self.data_profiling:\n",
    "            report = ProfileReport(df=df, minimal=False)\n",
    "            report.to_file(f\"{self.ideology}_test_{limit}_seed_{seed}\")\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def _run_test(self):\n",
    "        chat_prompt = ChatPromptTemplate.from_messages(\n",
    "            Generator.create_prompt_template(\n",
    "                self.prompt_dict[\"all\"].format(ideology=self.ideology)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        llm_chain = LLMChain(llm=self.local_llm, prompt=chat_prompt)\n",
    "        result = llm_chain.run(\n",
    "            ineffective_argument=\"Climate change \"\n",
    "            \"litigations are now linked to human rights. \"\n",
    "        )\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_examples(self, save: bool = True) -> list:\n",
    "        limit = Generator._LIMIT_ * self.fewshots_num_examples\n",
    "        seed = self.seed\n",
    "\n",
    "        name: str = f\"notaphoenix/debateorg_w_effect_for_{self.ideology}\"\n",
    "        dataset: Dataset = load_dataset(name, split=\"training\")\n",
    "        dataset = dataset.filter(\n",
    "            lambda x: x[\"label\"] == IESTAHuggingFace._LABEL2ID_[\"effective\"]\n",
    "            and len(x[\"text\"].split(\" \")) > 10\n",
    "            and len(x[\"text\"].split(\" \")) <= 1024\n",
    "            and detect(x[\"text\"]) == \"en\"\n",
    "        ).shuffle(seed=seed)\n",
    "\n",
    "        if len(dataset) > limit and not self.fewshots_w_semantic_similarity:\n",
    "            dataset = dataset.select(range(limit))\n",
    "        print(f\"{len(dataset)} new length\")\n",
    "        print(f\"Return dataset {name} with {len(dataset)} \")\n",
    "\n",
    "        df = dataset.to_pandas().copy()\n",
    "        filename: str = f\"{self.ideology}_training_{limit}_seed_{seed}_fewshot_{self.fewshots_num_examples}_similarity{self.fewshots_w_semantic_similarity}\"\n",
    "        #if self.trainingdata_profiling:\n",
    "           # report = ProfileReport(df=df, minimal=True)\n",
    "            #report.to_file(f\"data/{filename}.html\")\n",
    "\n",
    "        #df.to_csv(f\"data/{filename}.csv\")\n",
    "\n",
    "        result = [\n",
    "            {\"effective_argument\": x} for x in df[\"text\"].values.tolist()\n",
    "        ]\n",
    "        print(result[:3])\n",
    "        return result\n",
    "\n",
    "    # GENERATION #\n",
    "    # ---------- #\n",
    "    def generate_for_prompts(self, ineffective_argument: str):\n",
    "        result_dict = {}\n",
    "        print(\"generate_for_prompts called\")\n",
    "        # Preparing PROMPTS\n",
    "        local_examples = []\n",
    "        if self.use_fewshots:\n",
    "            if len(self.examples) < self.fewshots_num_examples:\n",
    "                print(\"warning: ran out of examples, replenishing...\") \n",
    "                self.examples = self.get_examples(save=False)\n",
    "\n",
    "            local_examples = [self.examples.pop() for _ in range(0, self.fewshots_num_examples)]\n",
    "\n",
    "        for k, prompt_template in self.prompt_dict.items():\n",
    "            if self.use_fewshots:\n",
    "                template = (\n",
    "                    prompt_template.format(ideology=self.ideology)\n",
    "                    + \"\\n    Argument: {ineffective_argument}\\n\"\n",
    "                )\n",
    "                # prompt = PromptTemplate(template=template, input_variables=[\"ineffective_argument\"])\n",
    "\n",
    "                example_prompt = PromptTemplate(\n",
    "                    input_variables=[\"effective_argument\"],\n",
    "                    template=\"An Example of an argument that has an effective style: {effective_argument}\",\n",
    "                )\n",
    "\n",
    "                prompt = FewShotPromptTemplate(\n",
    "                    examples=local_examples,\n",
    "                    example_prompt=example_prompt,\n",
    "                    suffix=template,\n",
    "                    input_variables=[\"ineffective_argument\"],\n",
    "                )\n",
    "            else:  # 0 shot\n",
    "                if self.model_name == Generator._MODEL_CHATGPT_:\n",
    "                    prompt = ChatPromptTemplate.from_messages(\n",
    "                        Generator.create_prompt_template(\n",
    "                            prompt_template.format(ideology=self.ideology)\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    template = (\n",
    "                        prompt_template.format(ideology=self.ideology)\n",
    "                        + \"\\n    Argument: {ineffective_argument}\\n \"\n",
    "                    )\n",
    "                    prompt = PromptTemplate(\n",
    "                        template=template,\n",
    "                        input_variables=[\"ineffective_argument\"],\n",
    "                    )\n",
    "\n",
    "            llm_chain = LLMChain(llm=self.local_llm, prompt=prompt)\n",
    "            result_dict[k] = llm_chain.run(\n",
    "                ineffective_argument=ineffective_argument\n",
    "            )\n",
    "            result_dict[f\"len_{k}\"] = len(result_dict[k])\n",
    "            result_dict[\"len_orig\"] = len(ineffective_argument)\n",
    "            if self.use_fewshots:\n",
    "                result_dict[\"examples\"] = local_examples\n",
    "\n",
    "        return result_dict\n",
    "\n",
    "    def generate_all(self):\n",
    "        fewshots_text = (\n",
    "            f\"_{self.fewshots_num_examples}fewshots\"\n",
    "            if self.use_fewshots\n",
    "            else \"\"\n",
    "        )\n",
    "        fewshots_text = (\n",
    "            f\"{fewshots_text}_with_similarity\"\n",
    "            if self.fewshots_w_semantic_similarity\n",
    "            else fewshots_text\n",
    "        )\n",
    "        out_file = f\"{self.out_file}{self.ideology}_{self.model_name.lower()}{fewshots_text}.jsonl\"\n",
    "\n",
    "        existing_indices = []\n",
    "        if exists(out_file):\n",
    "            _df = pd.read_json(path_or_buf=out_file, lines=True)\n",
    "            existing_indices = _df[\"idx\"].values.tolist()\n",
    "\n",
    "        add_new_l = False\n",
    "        if len(existing_indices) > 0:\n",
    "            print(f\"filtering out existing indices ({len(existing_indices)})\")\n",
    "            self.filtered_dataset = self.filtered_dataset.filter(\n",
    "                lambda example: example[\"idx\"] not in existing_indices\n",
    "            )\n",
    "            print(f\"{self.filtered_dataset.num_rows} to go...\")\n",
    "            add_new_l = True\n",
    "\n",
    "        with open(out_file, \"a\") as file:\n",
    "            for datapoint in tqdm(self.filtered_dataset):\n",
    "                try:\n",
    "                    promt_generated_dict = self.generate_for_prompts(\n",
    "                        datapoint[\"text\"]\n",
    "                    )\n",
    "                    promt_generated_dict.update(datapoint)\n",
    "                    nline = \"\\n\" if add_new_l else \"\"\n",
    "\n",
    "                    file.write(f\"{nline}{json.dumps(promt_generated_dict)}\")\n",
    "                    add_new_l = True\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print(\n",
    "                        f\"Failed to get a response for ID: {datapoint['idx']}\"\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m generator \u001b[39m=\u001b[39m Generator(\n\u001b[1;32m      2\u001b[0m                     ideology\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mliberal\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m                     model_name\u001b[39m=\u001b[39mGenerator\u001b[39m.\u001b[39m_MODEL_CHATGPT_,\n\u001b[1;32m      4\u001b[0m                     trainingdata_profiling\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m                     use_fewshots\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m                     fewshots_num_examples\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m      7\u001b[0m                     fewshots_w_semantic_similarity\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m                 )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Generator' is not defined"
     ]
    }
   ],
   "source": [
    "generator = Generator(\n",
    "                    ideology=\"liberal\",\n",
    "                    model_name=Generator._MODEL_CHATGPT_,\n",
    "                    trainingdata_profiling=True,\n",
    "                    use_fewshots=True,\n",
    "                    fewshots_num_examples=1,\n",
    "                    fewshots_w_semantic_similarity=False,\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.generate_for_prompts(\"Climate changes is not caused by humans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.generate_for_prompts(\"THIS IS AN INEFFECTIVE ARGUMENT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "found = load_dotenv(f\"../.env\")\n",
    "\n",
    "seed = 2062021\n",
    "name: str = f\"notaphoenix/debateorg_w_effect_for_conservative\"\n",
    "dataset: Dataset = load_dataset(name, split=\"test\")\n",
    "dataset = dataset.filter(\n",
    "    lambda x: x[\"label\"] == 0\n",
    ").shuffle(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "counts    134\n",
       "Name: Politics, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_counts.loc[\"Politics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politics: 134\n",
      "Society: 72\n",
      "Religion: 70\n",
      "Science: 43\n",
      "Miscellaneous: 31\n",
      "Philosophy: 26\n",
      "Education: 25\n",
      "Entertainment: 20\n",
      "Health: 16\n",
      "Games: 12\n",
      "Sports: 11\n",
      "Economics: 8\n",
      "Funny: 7\n",
      "Arts: 7\n",
      "Technology: 6\n",
      "News: 5\n",
      "People: 3\n",
      "Movies: 2\n",
      "TV: 1\n",
      "Music: 1\n"
     ]
    }
   ],
   "source": [
    "category_counts = test_data.to_pandas()[\"category\"].value_counts().to_frame('counts')\n",
    "for category, count_row in category_counts.iterrows():\n",
    "    print(f\"{category}: {count_row['counts']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration notaphoenix--debateorg_w_effect_for_liberal-d0ad1689381f171f\n",
      "Found cached dataset parquet (/home/elba_ro/.cache/huggingface/datasets/notaphoenix___parquet/notaphoenix--debateorg_w_effect_for_liberal-d0ad1689381f171f/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010867834091186523,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 4,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb1fca9c8154b47911a270210c4138f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 before len filter\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00463104248046875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b6598be39f443bb7ccb2a728838fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "415 after len filter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration notaphoenix--debateorg_w_effect_for_liberal-d0ad1689381f171f\n",
      "Found cached dataset parquet (/home/elba_ro/.cache/huggingface/datasets/notaphoenix___parquet/notaphoenix--debateorg_w_effect_for_liberal-d0ad1689381f171f/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010445594787597656,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 4,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06699ce127f4425288e8739ca1af7271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012933492660522461,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 4,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c9b01328aea4f6698e73d69026b020f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 of extra\n",
      "500 new length\n",
      "Return dataset notaphoenix/debateorg_w_effect_for_liberal with 500 \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(effect=\"ineffective\", ideology=\"liberal\"):\n",
    "        limit = 500\n",
    "        seed = 2062021\n",
    "        name: str = f\"notaphoenix/debateorg_w_effect_for_{ideology}\"\n",
    "        dataset: Dataset = load_dataset(name, split=\"test\")\n",
    "        dataset = dataset.filter(\n",
    "            lambda x: x[\"label\"] == IESTAHuggingFace._LABEL2ID_[effect]\n",
    "        ).shuffle(seed=seed)\n",
    "\n",
    "        if len(dataset) > limit:\n",
    "            dataset = dataset.select(range(limit))\n",
    "\n",
    "        print(f\"{len(dataset)} before len filter\")\n",
    "        dataset = dataset.filter(\n",
    "            lambda x: len(x[\"text\"].split(\" \")) > 10\n",
    "            and len(x[\"text\"].split(\" \")) <= 1024\n",
    "            and x[\"idx\"] != 64707\n",
    "            and detect(x[\"text\"]) == \"en\"\n",
    "        )\n",
    "        print(f\"{len(dataset)} after len filter\")\n",
    "\n",
    "        while len(dataset) < limit:\n",
    "            idxes = dataset.to_pandas()[\"idx\"].values.tolist()\n",
    "            dataset_extra: Dataset = load_dataset(name, split=\"test\")\n",
    "            dataset_extra = dataset_extra.filter(\n",
    "                lambda x: len(x[\"text\"].split(\" \")) > 10\n",
    "                and len(x[\"text\"].split(\" \")) <= 1024\n",
    "                and x[\"idx\"] != 64707\n",
    "                and detect(x[\"text\"]) == \"en\"\n",
    "            )\n",
    "\n",
    "            dataset_extra = dataset_extra.filter(\n",
    "                lambda x: x[\"label\"] == IESTAHuggingFace._LABEL2ID_[effect]\n",
    "                and [\"idx\"] not in idxes\n",
    "            ).shuffle(seed=seed)\n",
    "            dataset_extra = dataset_extra.select(range(limit - len(dataset)))\n",
    "            print(f\"{len(dataset_extra)} of extra\")\n",
    "            dataset = concatenate_datasets([dataset, dataset_extra])\n",
    "\n",
    "            print(f\"{len(dataset)} new length\")\n",
    "        print(f\"Return dataset {name} with {len(dataset)} \")\n",
    "        # dataset = dataset.map(lambda example, idx: {\"id\": idx, **example}, with_indices=True)\n",
    "\n",
    "        df = dataset.to_pandas().copy()\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iesta_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
