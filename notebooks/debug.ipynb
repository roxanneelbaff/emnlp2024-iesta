{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "\n",
    "    pipeline,\n",
    ")\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dotenv was True\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = LlamaTokenizer.from_pretrained(\"chavinlo/alpaca-native\")\n",
    "found = load_dotenv(find_dotenv())\n",
    "print(f\"dotenv was {found}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "ALPACA_WEIGHTS_FOLDER = \"/localdata1/EmEx/model_weights/alpaca_7b\"\n",
    "\n",
    "alpaca_model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    ALPACA_WEIGHTS_FOLDER\n",
    ").to(device)\n",
    "alpaca_tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    ALPACA_WEIGHTS_FOLDER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=alpaca_model,\n",
    "    tokenizer=alpaca_tokenizer,\n",
    "    max_length=1024,\n",
    "    # temperature=0.2,\n",
    "    # top_p=0.95,\n",
    "    # repetition_penalty=1.2\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAIChat\n",
    "from langchain import LLMChain\n",
    "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "from ast import literal_eval\n",
    "import random\n",
    "import re\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "basic_str = \"Transform the following argument to an effective argument by maintaining the original length\"\n",
    "ideology_str = \"for readers with a {ideology} political ideology\"\n",
    "content_str = \"by preserving the content of the argument\"\n",
    "style_str = \"by only changing the style of the text\"\n",
    "\n",
    "prompt_dict = {\n",
    "    \"basic\": f\"{basic_str}:\",\n",
    "    \"ideology\": f\"{basic_str} {ideology_str}:\",\n",
    "    \"content\": f\"{basic_str} {content_str}:\",\n",
    "    \"style\": f\"{basic_str} {style_str}:\",\n",
    "    \"ideology-content\": f\"{basic_str} {ideology_str} {content_str}:\",\n",
    "    \"ideology-style\": f\"{basic_str} {ideology_str} {style_str}:\",\n",
    "    \"all\": f\"{basic_str} {ideology_str} {content_str} and {style_str}:\",\n",
    "}\n",
    "\n",
    "\n",
    "def create_prompt_template(prompt):\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(prompt)\n",
    "    human_template = \"{ineffective_argument}\"\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
    "        human_template\n",
    "    )\n",
    "    return [system_message_prompt, human_message_prompt]\n",
    "\n",
    "\n",
    "liberal_chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    create_prompt_template(prompt_dict[\"all\"].format(ideology=\"liberal\"))\n",
    ")\n",
    "llm_chain = LLMChain(llm=local_llm, prompt=liberal_chat_prompt)\n",
    "result = llm_chain.run(\n",
    "    ineffective_argument=\"If there was no Kryptonite, can Superman defeat the Silver Surfer?\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nalpaca_llm = AutoModelForCausalLM.from_pretrained(\\n    ALPACA_WEIGHTS_FOLDER,\\n    # load_in_8bit=True,\\n    device_map=\"auto\",\\n)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "ALPACA_WEIGHTS_FOLDER = \"/localdata1/EmEx/model_weights/alpaca_7b\"\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(ALPACA_WEIGHTS_FOLDER)\n",
    "'''\n",
    "alpaca_llm = AutoModelForCausalLM.from_pretrained(\n",
    "    ALPACA_WEIGHTS_FOLDER,\n",
    "    # load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=alpaca_llm,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=1024,\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2,\n",
    ")\n",
    "local_model = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "template = \"\"\"Create an argument for Conservative readers related to a specific topic. Return at least 200 words.\n",
    "\n",
    "Topic:\n",
    "{topic}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"topic\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=local_model)\n",
    "qn = \"climate change\"\n",
    "print(llm_chain.run(qn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q install -i https://test.pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install cudatoolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall -y bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q install -U bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q uninstall -y bitsandbytes-cuda114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q uninstall -y bitsandbytes-cuda117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show iesta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dotenv was True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elba_ro/repos/github/conf22-style-transfer/.venv/lib/python3.9/site-packages/numba/core/decorators.py:262: NumbaDeprecationWarning: numba.generated_jit is deprecated. Please see the documentation at: https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-generated-jit for more information and advice on a suitable replacement.\n",
      "  warnings.warn(msg, NumbaDeprecationWarning)\n",
      "/home/elba_ro/repos/github/conf22-style-transfer/.venv/lib/python3.9/site-packages/visions/backends/shared/nan_handling.py:51: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def hasna(x: np.ndarray) -> bool:\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from iesta.data.huggingface_loader import IESTAHuggingFace\n",
    "from ydata_profiling import ProfileReport\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from os.path import exists\n",
    "import dataclasses\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from datasets.combine import concatenate_datasets\n",
    "from iesta.data.huggingface_loader import IESTAHuggingFace\n",
    "from ydata_profiling import ProfileReport\n",
    "from langdetect import detect\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM \n",
    "from typing import ClassVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m generator \u001b[39m=\u001b[39m Generator(\n\u001b[1;32m      2\u001b[0m                     ideology\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mliberal\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m                     model_name\u001b[39m=\u001b[39mGenerator\u001b[39m.\u001b[39m_MODEL_CHATGPT_,\n\u001b[1;32m      4\u001b[0m                     trainingdata_profiling\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m                     use_fewshots\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m                     fewshots_num_examples\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m      7\u001b[0m                     fewshots_w_semantic_similarity\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m                 )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Generator' is not defined"
     ]
    }
   ],
   "source": [
    "generator = Generator(\n",
    "                    ideology=\"liberal\",\n",
    "                    model_name=Generator._MODEL_CHATGPT_,\n",
    "                    use_fewshots=True,\n",
    "                    n_shots=1,\n",
    "                    fewshots_w_semantic_similarity=False,\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.generate_for_prompts(\"Climate changes is not caused by humans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.generate_for_prompts(\"THIS IS AN INEFFECTIVE ARGUMENT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "found = load_dotenv(f\"../.env\")\n",
    "\n",
    "seed = 2062021\n",
    "name: str = f\"notaphoenix/debateorg_w_effect_for_conservative\"\n",
    "dataset: Dataset = load_dataset(name, split=\"test\")\n",
    "dataset = dataset.filter(\n",
    "    lambda x: x[\"label\"] == 0\n",
    ").shuffle(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "counts    134\n",
       "Name: Politics, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_counts.loc[\"Politics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politics: 134\n",
      "Society: 72\n",
      "Religion: 70\n",
      "Science: 43\n",
      "Miscellaneous: 31\n",
      "Philosophy: 26\n",
      "Education: 25\n",
      "Entertainment: 20\n",
      "Health: 16\n",
      "Games: 12\n",
      "Sports: 11\n",
      "Economics: 8\n",
      "Funny: 7\n",
      "Arts: 7\n",
      "Technology: 6\n",
      "News: 5\n",
      "People: 3\n",
      "Movies: 2\n",
      "TV: 1\n",
      "Music: 1\n"
     ]
    }
   ],
   "source": [
    "category_counts = test_data.to_pandas()[\"category\"].value_counts().to_frame('counts')\n",
    "for category, count_row in category_counts.iterrows():\n",
    "    print(f\"{category}: {count_row['counts']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration notaphoenix--debateorg_w_effect_for_liberal-d0ad1689381f171f\n",
      "Found cached dataset parquet (/home/elba_ro/.cache/huggingface/datasets/notaphoenix___parquet/notaphoenix--debateorg_w_effect_for_liberal-d0ad1689381f171f/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010867834091186523,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 4,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb1fca9c8154b47911a270210c4138f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 before len filter\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00463104248046875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b6598be39f443bb7ccb2a728838fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "415 after len filter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration notaphoenix--debateorg_w_effect_for_liberal-d0ad1689381f171f\n",
      "Found cached dataset parquet (/home/elba_ro/.cache/huggingface/datasets/notaphoenix___parquet/notaphoenix--debateorg_w_effect_for_liberal-d0ad1689381f171f/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010445594787597656,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 4,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06699ce127f4425288e8739ca1af7271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012933492660522461,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 4,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c9b01328aea4f6698e73d69026b020f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 of extra\n",
      "500 new length\n",
      "Return dataset notaphoenix/debateorg_w_effect_for_liberal with 500 \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(effect=\"ineffective\", ideology=\"liberal\"):\n",
    "        limit = 500\n",
    "        seed = 2062021\n",
    "        name: str = f\"notaphoenix/debateorg_w_effect_for_{ideology}\"\n",
    "        dataset: Dataset = load_dataset(name, split=\"test\")\n",
    "        dataset = dataset.filter(\n",
    "            lambda x: x[\"label\"] == IESTAHuggingFace._LABEL2ID_[effect]\n",
    "        ).shuffle(seed=seed)\n",
    "\n",
    "        if len(dataset) > limit:\n",
    "            dataset = dataset.select(range(limit))\n",
    "\n",
    "        print(f\"{len(dataset)} before len filter\")\n",
    "        dataset = dataset.filter(\n",
    "            lambda x: len(x[\"text\"].split(\" \")) > 10\n",
    "            and len(x[\"text\"].split(\" \")) <= 1024\n",
    "            and x[\"idx\"] != 64707\n",
    "            and detect(x[\"text\"]) == \"en\"\n",
    "        )\n",
    "        print(f\"{len(dataset)} after len filter\")\n",
    "\n",
    "        while len(dataset) < limit:\n",
    "            idxes = dataset.to_pandas()[\"idx\"].values.tolist()\n",
    "            dataset_extra: Dataset = load_dataset(name, split=\"test\")\n",
    "            dataset_extra = dataset_extra.filter(\n",
    "                lambda x: len(x[\"text\"].split(\" \")) > 10\n",
    "                and len(x[\"text\"].split(\" \")) <= 1024\n",
    "                and x[\"idx\"] != 64707\n",
    "                and detect(x[\"text\"]) == \"en\"\n",
    "            )\n",
    "\n",
    "            dataset_extra = dataset_extra.filter(\n",
    "                lambda x: x[\"label\"] == IESTAHuggingFace._LABEL2ID_[effect]\n",
    "                and [\"idx\"] not in idxes\n",
    "            ).shuffle(seed=seed)\n",
    "            dataset_extra = dataset_extra.select(range(limit - len(dataset)))\n",
    "            print(f\"{len(dataset_extra)} of extra\")\n",
    "            dataset = concatenate_datasets([dataset, dataset_extra])\n",
    "\n",
    "            print(f\"{len(dataset)} new length\")\n",
    "        print(f\"Return dataset {name} with {len(dataset)} \")\n",
    "        # dataset = dataset.map(lambda example, idx: {\"id\": idx, **example}, with_indices=True)\n",
    "\n",
    "        df = dataset.to_pandas().copy()\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iesta_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
