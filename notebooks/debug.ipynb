{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    LlamaTokenizer,\n",
    "    LlamaForCausalLM,\n",
    "    GenerationConfig,\n",
    "    pipeline,\n",
    ")\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dotenv was True\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = LlamaTokenizer.from_pretrained(\"chavinlo/alpaca-native\")\n",
    "found = load_dotenv(find_dotenv())\n",
    "print(f\"dotenv was {found}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "ALPACA_WEIGHTS_FOLDER = \"/localdata1/EmEx/model_weights/alpaca_7b\"\n",
    "\n",
    "alpaca_model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    ALPACA_WEIGHTS_FOLDER\n",
    ").to(device)\n",
    "alpaca_tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    ALPACA_WEIGHTS_FOLDER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=alpaca_model,\n",
    "    tokenizer=alpaca_tokenizer,\n",
    "    max_length=1024,\n",
    "    # temperature=0.2,\n",
    "    # top_p=0.95,\n",
    "    # repetition_penalty=1.2\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAIChat\n",
    "from langchain import LLMChain\n",
    "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "from ast import literal_eval\n",
    "import random\n",
    "import re\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "basic_str = \"Transform the following argument to an effective argument by maintaining the original length\"\n",
    "ideology_str = \"for readers with a {ideology} political ideology\"\n",
    "content_str = \"by preserving the content of the argument\"\n",
    "style_str = \"by only changing the style of the text\"\n",
    "\n",
    "prompt_dict = {\n",
    "    \"basic\": f\"{basic_str}:\",\n",
    "    \"ideology\": f\"{basic_str} {ideology_str}:\",\n",
    "    \"content\": f\"{basic_str} {content_str}:\",\n",
    "    \"style\": f\"{basic_str} {style_str}:\",\n",
    "    \"ideology-content\": f\"{basic_str} {ideology_str} {content_str}:\",\n",
    "    \"ideology-style\": f\"{basic_str} {ideology_str} {style_str}:\",\n",
    "    \"all\": f\"{basic_str} {ideology_str} {content_str} and {style_str}:\",\n",
    "}\n",
    "\n",
    "\n",
    "def create_prompt_template(prompt):\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(prompt)\n",
    "    human_template = \"{ineffective_argument}\"\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
    "        human_template\n",
    "    )\n",
    "    return [system_message_prompt, human_message_prompt]\n",
    "\n",
    "\n",
    "liberal_chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    create_prompt_template(prompt_dict[\"all\"].format(ideology=\"liberal\"))\n",
    ")\n",
    "llm_chain = LLMChain(llm=local_llm, prompt=liberal_chat_prompt)\n",
    "result = llm_chain.run(\n",
    "    ineffective_argument=\"If there was no Kryptonite, can Superman defeat the Silver Surfer?\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a5f74806a248868257307d094bace8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elba_ro/repos/github/conf22-style-transfer/iesta_venv/lib/python3.8/site-packages/transformers/generation/utils.py:2415: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /home/elba_ro/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1442.)\n",
      "  next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The boiling point of water at standard pressure is 100°C (212°F).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "ALPACA_WEIGHTS_FOLDER = \"/localdata1/EmEx/model_weights/alpaca_7b\"\n",
    "from transformers import (\n",
    "    LlamaForCausalLM,\n",
    "    LlamaTokenizer,\n",
    "    GenerationConfig,\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ALPACA_WEIGHTS_FOLDER)\n",
    "\n",
    "alpaca_llm = AutoModelForCausalLM.from_pretrained(\n",
    "    ALPACA_WEIGHTS_FOLDER,\n",
    "    # load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Climate change is real and man-made, and it's up to us to take action now in order to mitigate its effects on our planet. We must reduce emissions by investing in renewable energy sources like wind and solar power, as well as implementing policies that encourage sustainability. Additionally, we should be doing more to protect the environment through conservation efforts such as reforestation and reducing pollution from industrial processes. If we don’t act soon, climate change could cause irreversible damage to our world, so it’s important that we make responsible decisions today.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=alpaca_llm,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=1024,\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2,\n",
    ")\n",
    "local_model = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "template = \"\"\"Create an argument for Conservative readers related to a specific topic. Return at least 200 words.\n",
    "\n",
    "Topic:\n",
    "{topic}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"topic\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=local_model)\n",
    "qn = \"climate change\"\n",
    "print(llm_chain.run(qn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q install -i https://test.pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install cudatoolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall -y bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q install -U bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q uninstall -y bitsandbytes-cuda114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q uninstall -y bitsandbytes-cuda117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q install -U bitsandbytes-cuda117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q install -U bitsandbytes-cuda117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iesta_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
