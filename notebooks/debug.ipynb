{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    LlamaTokenizer,\n",
    "    LlamaForCausalLM,\n",
    "    GenerationConfig,\n",
    "    pipeline,\n",
    ")\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = LlamaTokenizer.from_pretrained(\"chavinlo/alpaca-native\")\n",
    "found = load_dotenv(find_dotenv())\n",
    "print(f\"dotenv was {found}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "ALPACA_WEIGHTS_FOLDER = \"/localdata1/EmEx/model_weights/alpaca_7b\"\n",
    "\n",
    "alpaca_model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    ALPACA_WEIGHTS_FOLDER\n",
    ").to(device)\n",
    "alpaca_tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    ALPACA_WEIGHTS_FOLDER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=alpaca_model,\n",
    "    tokenizer=alpaca_tokenizer,\n",
    "    max_length=1024,\n",
    "    # temperature=0.2,\n",
    "    # top_p=0.95,\n",
    "    # repetition_penalty=1.2\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAIChat\n",
    "from langchain import LLMChain\n",
    "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "from ast import literal_eval\n",
    "import random\n",
    "import re\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "basic_str = \"Transform the following argument to an effective argument by maintaining the original length\"\n",
    "ideology_str = \"for readers with a {ideology} political ideology\"\n",
    "content_str = \"by preserving the content of the argument\"\n",
    "style_str = \"by only changing the style of the text\"\n",
    "\n",
    "prompt_dict = {\n",
    "    \"basic\": f\"{basic_str}:\",\n",
    "    \"ideology\": f\"{basic_str} {ideology_str}:\",\n",
    "    \"content\": f\"{basic_str} {content_str}:\",\n",
    "    \"style\": f\"{basic_str} {style_str}:\",\n",
    "    \"ideology-content\": f\"{basic_str} {ideology_str} {content_str}:\",\n",
    "    \"ideology-style\": f\"{basic_str} {ideology_str} {style_str}:\",\n",
    "    \"all\": f\"{basic_str} {ideology_str} {content_str} and {style_str}:\",\n",
    "}\n",
    "\n",
    "\n",
    "def create_prompt_template(prompt):\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(prompt)\n",
    "    human_template = \"{ineffective_argument}\"\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
    "        human_template\n",
    "    )\n",
    "    return [system_message_prompt, human_message_prompt]\n",
    "\n",
    "\n",
    "liberal_chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    create_prompt_template(prompt_dict[\"all\"].format(ideology=\"liberal\"))\n",
    ")\n",
    "llm_chain = LLMChain(llm=local_llm, prompt=liberal_chat_prompt)\n",
    "result = llm_chain.run(\n",
    "    ineffective_argument=\"If there was no Kryptonite, can Superman defeat the Silver Surfer?\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "ALPACA_WEIGHTS_FOLDER = \"/localdata1/EmEx/model_weights/alpaca_7b\"\n",
    "from transformers import (\n",
    "    LlamaForCausalLM,\n",
    "    LlamaTokenizer,\n",
    "    GenerationConfig,\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ALPACA_WEIGHTS_FOLDER)\n",
    "\n",
    "alpaca_llm = AutoModelForCausalLM.from_pretrained(\n",
    "    ALPACA_WEIGHTS_FOLDER,\n",
    "    # load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=alpaca_llm,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=1024,\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2,\n",
    ")\n",
    "local_model = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "template = \"\"\"Create an argument for Conservative readers related to a specific topic. Return at least 200 words.\n",
    "\n",
    "Topic:\n",
    "{topic}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"topic\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=local_model)\n",
    "qn = \"climate change\"\n",
    "print(llm_chain.run(qn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q install -i https://test.pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install cudatoolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall -y bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q install -U bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q uninstall -y bitsandbytes-cuda114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q uninstall -y bitsandbytes-cuda117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show iesta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import transformers\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from iesta.machine_learning.huggingface_loader import IESTAHuggingFace\n",
    "from ydata_profiling import ProfileReport\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from os.path import exists\n",
    "import dataclasses\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from datasets.combine import concatenate_datasets\n",
    "from iesta.machine_learning.huggingface_loader import IESTAHuggingFace\n",
    "from ydata_profiling import ProfileReport\n",
    "from langdetect import detect\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM \n",
    "from typing import ClassVar\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Generator:\n",
    "    ideology: str  # liberal or conservative\n",
    "    model_name: str  # gpt or alpaca\n",
    "\n",
    "    data_limit: int = 500\n",
    "    data_profiling: bool = False\n",
    "    data_save: bool = False\n",
    "    seed: int = 2062021\n",
    "\n",
    "    out_file: str = \"data/llms_out/\"\n",
    "\n",
    "    use_fewshots: bool = False\n",
    "    fewshots_num_examples: int = 1  # we use 1 to 3\n",
    "    fewshots_w_semantic_similarity: bool = False\n",
    "    verbose: int = 0\n",
    "    trainingdata_profiling: bool = True\n",
    "    _MODEL_CHATGPT_: ClassVar = \"gpt-3.5-turbo\"\n",
    "    _MODEL_ALPACA_: ClassVar = \"alpaca\"\n",
    "    _LIMIT_: ClassVar = 500\n",
    "\n",
    "    # HELPERS   #\n",
    "    # --------- #\n",
    "    @staticmethod\n",
    "    def init_prompts():\n",
    "        basic_str = \"Transform the following argument to an effective argument by maintaining the original length\"\n",
    "        ideology_str = \"for readers with a {ideology} political ideology\"\n",
    "        content_str = \"by preserving the content of the argument\"\n",
    "        style_str = \"by only changing the style of the text\"\n",
    "\n",
    "        prompt_dict = {\n",
    "            \"basic\": f\"{basic_str}:\",\n",
    "            \"ideology\": f\"{basic_str} {ideology_str}:\",\n",
    "            \"content\": f\"{basic_str}, and {content_str}:\",\n",
    "            \"style\": f\"{basic_str}, and {style_str}:\",\n",
    "            \"ideology-content\": f\"{basic_str} {ideology_str} {content_str}:\",\n",
    "            \"ideology-style\": f\"{basic_str} {ideology_str} {style_str}:\",\n",
    "            \"all\": f\"{basic_str} {ideology_str} {content_str}, and {style_str}:\",\n",
    "        }\n",
    "        return prompt_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def create_prompt_template(prompt):\n",
    "        system_message_prompt = SystemMessagePromptTemplate.from_template(\n",
    "            prompt\n",
    "        )\n",
    "        human_template = \"Argument: {ineffective_argument}\"\n",
    "        human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
    "            human_template\n",
    "        )\n",
    "\n",
    "        return [system_message_prompt, human_message_prompt]\n",
    "\n",
    "    # ---- End Helpers --- #\n",
    "\n",
    "    def __post_init__(self):\n",
    "        found = load_dotenv(\"/home/elba_ro/repos/github/conf22-style-transfer/.env\")\n",
    "        print(f\"dotenv was found: {found}\")\n",
    "\n",
    "        print(\"Initializing all prompt templates in variable prompt_dict..\")\n",
    "        self.prompt_dict = Generator.init_prompts()\n",
    "\n",
    "        print(\n",
    "            f\"Initializing LLM for {self.model_name} in variable local_llm...\"\n",
    "        )\n",
    "        self.local_llm = self.get_model()\n",
    "\n",
    "        print(\n",
    "            f\"Getting filtered dataset top {self.data_limit} in variable\"\n",
    "            \"filtered_dataset...\"\n",
    "        )\n",
    "        self.filtered_dataset = self.get_data(effect=\"ineffective\")\n",
    "\n",
    "        if self.use_fewshots:\n",
    "            self.examples = self.get_examples()\n",
    "\n",
    "    def get_model(self):\n",
    "        if self.model_name == Generator._MODEL_CHATGPT_:\n",
    "            local_llm = ChatOpenAI(\n",
    "                model_name=Generator._MODEL_CHATGPT_, temperature=0\n",
    "            )\n",
    "        elif self.model_name == Generator._MODEL_ALPACA_:\n",
    "            # device = torch.device('cuda:0')\n",
    "            ALPACA_WEIGHTS_FOLDER = \"/localdata1/EmEx/model_weights/alpaca_7b\"\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained(ALPACA_WEIGHTS_FOLDER)\n",
    "            alpaca_llm = AutoModelForCausalLM.from_pretrained(\n",
    "                ALPACA_WEIGHTS_FOLDER,\n",
    "                # load_in_8bit=True,\n",
    "                device_map=\"cuda:1\",\n",
    "            )\n",
    "\n",
    "            pipe = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=alpaca_llm,\n",
    "                tokenizer=tokenizer,\n",
    "                max_length=4096,\n",
    "                temperature=0,\n",
    "                top_p=0.95,\n",
    "                repetition_penalty=1.2,\n",
    "            )\n",
    "            local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "        return local_llm\n",
    "\n",
    "    def get_data(self, effect=\"ineffective\"):\n",
    "        limit = Generator._LIMIT_\n",
    "        seed = 2062021\n",
    "        name: str = f\"notaphoenix/debateorg_w_effect_for_{self.ideology}\"\n",
    "        dataset: Dataset = load_dataset(name, split=\"test\")\n",
    "        dataset = dataset.filter(\n",
    "            lambda x: x[\"label\"] == IESTAHuggingFace._LABEL2ID_[effect]\n",
    "        ).shuffle(seed=seed)\n",
    "\n",
    "        if len(dataset) > limit:\n",
    "            dataset = dataset.select(range(limit))\n",
    "\n",
    "        print(f\"{len(dataset)} before len filter\")\n",
    "        dataset = dataset.filter(\n",
    "            lambda x: len(x[\"text\"].split(\" \")) > 10\n",
    "            and len(x[\"text\"].split(\" \")) <= 1024\n",
    "            and x[\"idx\"] != 64707\n",
    "            and detect(x[\"text\"]) == \"en\"\n",
    "        )\n",
    "        print(f\"{len(dataset)} after len filter\")\n",
    "\n",
    "        while len(dataset) < limit:\n",
    "            idxes = dataset.to_pandas()[\"idx\"].values.tolist()\n",
    "            dataset_extra: Dataset = load_dataset(name, split=\"test\")\n",
    "            dataset_extra = dataset_extra.filter(\n",
    "                lambda x: len(x[\"text\"].split(\" \")) > 10\n",
    "                and len(x[\"text\"].split(\" \")) <= 1024\n",
    "                and x[\"idx\"] != 64707\n",
    "                and detect(x[\"text\"]) == \"en\"\n",
    "            )\n",
    "\n",
    "            dataset_extra = dataset_extra.filter(\n",
    "                lambda x: x[\"label\"] == IESTAHuggingFace._LABEL2ID_[effect]\n",
    "                and [\"idx\"] not in idxes\n",
    "            ).shuffle(seed=seed)\n",
    "            dataset_extra = dataset_extra.select(range(limit - len(dataset)))\n",
    "            print(f\"{len(dataset_extra)} of extra\")\n",
    "            dataset = concatenate_datasets([dataset, dataset_extra])\n",
    "\n",
    "            print(f\"{len(dataset)} new length\")\n",
    "        print(f\"Return dataset {name} with {len(dataset)} \")\n",
    "        # dataset = dataset.map(lambda example, idx: {\"id\": idx, **example}, with_indices=True)\n",
    "\n",
    "        df = dataset.to_pandas().copy()\n",
    "        if self.data_profiling:\n",
    "            report = ProfileReport(df=df, minimal=False)\n",
    "            report.to_file(f\"{self.ideology}_test_{limit}_seed_{seed}\")\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def _run_test(self):\n",
    "        chat_prompt = ChatPromptTemplate.from_messages(\n",
    "            Generator.create_prompt_template(\n",
    "                self.prompt_dict[\"all\"].format(ideology=self.ideology)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        llm_chain = LLMChain(llm=self.local_llm, prompt=chat_prompt)\n",
    "        result = llm_chain.run(\n",
    "            ineffective_argument=\"Climate change \"\n",
    "            \"litigations are now linked to human rights. \"\n",
    "        )\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_examples(self, save: bool = True) -> list:\n",
    "        limit = Generator._LIMIT_ * self.fewshots_num_examples\n",
    "        seed = self.seed\n",
    "\n",
    "        name: str = f\"notaphoenix/debateorg_w_effect_for_{self.ideology}\"\n",
    "        dataset: Dataset = load_dataset(name, split=\"training\")\n",
    "        dataset = dataset.filter(\n",
    "            lambda x: x[\"label\"] == IESTAHuggingFace._LABEL2ID_[\"effective\"]\n",
    "            and len(x[\"text\"].split(\" \")) > 10\n",
    "            and len(x[\"text\"].split(\" \")) <= 1024\n",
    "            and detect(x[\"text\"]) == \"en\"\n",
    "        ).shuffle(seed=seed)\n",
    "\n",
    "        if len(dataset) > limit and not self.fewshots_w_semantic_similarity:\n",
    "            dataset = dataset.select(range(limit))\n",
    "        print(f\"{len(dataset)} new length\")\n",
    "        print(f\"Return dataset {name} with {len(dataset)} \")\n",
    "\n",
    "        df = dataset.to_pandas().copy()\n",
    "        filename: str = f\"{self.ideology}_training_{limit}_seed_{seed}_fewshot_{self.fewshots_num_examples}_similarity{self.fewshots_w_semantic_similarity}\"\n",
    "        #if self.trainingdata_profiling:\n",
    "           # report = ProfileReport(df=df, minimal=True)\n",
    "            #report.to_file(f\"data/{filename}.html\")\n",
    "\n",
    "        #df.to_csv(f\"data/{filename}.csv\")\n",
    "\n",
    "        result = [\n",
    "            {\"effective_argument\": x} for x in df[\"text\"].values.tolist()\n",
    "        ]\n",
    "        print(result[:3])\n",
    "        return result\n",
    "\n",
    "    # GENERATION #\n",
    "    # ---------- #\n",
    "    def generate_for_prompts(self, ineffective_argument: str):\n",
    "        result_dict = {}\n",
    "        print(\"generate_for_prompts called\")\n",
    "        # Preparing PROMPTS\n",
    "        local_examples = []\n",
    "        if self.use_fewshots:\n",
    "            if len(self.examples) < self.fewshots_num_examples:\n",
    "                print(\"warning: ran out of examples, replenishing...\") \n",
    "                self.examples = self.get_examples(save=False)\n",
    "\n",
    "            local_examples = [self.examples.pop() for _ in range(0, self.fewshots_num_examples)]\n",
    "\n",
    "        for k, prompt_template in self.prompt_dict.items():\n",
    "            if self.use_fewshots:\n",
    "                template = (\n",
    "                    prompt_template.format(ideology=self.ideology)\n",
    "                    + \"\\n    Argument: {ineffective_argument}\\n\"\n",
    "                )\n",
    "                # prompt = PromptTemplate(template=template, input_variables=[\"ineffective_argument\"])\n",
    "\n",
    "                example_prompt = PromptTemplate(\n",
    "                    input_variables=[\"effective_argument\"],\n",
    "                    template=\"An Example of an argument that has an effective style: {effective_argument}\",\n",
    "                )\n",
    "\n",
    "                prompt = FewShotPromptTemplate(\n",
    "                    examples=local_examples,\n",
    "                    example_prompt=example_prompt,\n",
    "                    suffix=template,\n",
    "                    input_variables=[\"ineffective_argument\"],\n",
    "                )\n",
    "            else:  # 0 shot\n",
    "                if self.model_name == Generator._MODEL_CHATGPT_:\n",
    "                    prompt = ChatPromptTemplate.from_messages(\n",
    "                        Generator.create_prompt_template(\n",
    "                            prompt_template.format(ideology=self.ideology)\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    template = (\n",
    "                        prompt_template.format(ideology=self.ideology)\n",
    "                        + \"\\n    Argument: {ineffective_argument}\\n \"\n",
    "                    )\n",
    "                    prompt = PromptTemplate(\n",
    "                        template=template,\n",
    "                        input_variables=[\"ineffective_argument\"],\n",
    "                    )\n",
    "\n",
    "            llm_chain = LLMChain(llm=self.local_llm, prompt=prompt)\n",
    "            result_dict[k] = llm_chain.run(\n",
    "                ineffective_argument=ineffective_argument\n",
    "            )\n",
    "            result_dict[f\"len_{k}\"] = len(result_dict[k])\n",
    "            result_dict[\"len_orig\"] = len(ineffective_argument)\n",
    "            if self.use_fewshots:\n",
    "                result_dict[\"examples\"] = local_examples\n",
    "\n",
    "        return result_dict\n",
    "\n",
    "    def generate_all(self):\n",
    "        fewshots_text = (\n",
    "            f\"_{self.fewshots_num_examples}fewshots\"\n",
    "            if self.use_fewshots\n",
    "            else \"\"\n",
    "        )\n",
    "        fewshots_text = (\n",
    "            f\"{fewshots_text}_with_similarity\"\n",
    "            if self.fewshots_w_semantic_similarity\n",
    "            else fewshots_text\n",
    "        )\n",
    "        out_file = f\"{self.out_file}{self.ideology}_{self.model_name.lower()}{fewshots_text}.jsonl\"\n",
    "\n",
    "        existing_indices = []\n",
    "        if exists(out_file):\n",
    "            _df = pd.read_json(path_or_buf=out_file, lines=True)\n",
    "            existing_indices = _df[\"idx\"].values.tolist()\n",
    "\n",
    "        add_new_l = False\n",
    "        if len(existing_indices) > 0:\n",
    "            print(f\"filtering out existing indices ({len(existing_indices)})\")\n",
    "            self.filtered_dataset = self.filtered_dataset.filter(\n",
    "                lambda example: example[\"idx\"] not in existing_indices\n",
    "            )\n",
    "            print(f\"{self.filtered_dataset.num_rows} to go...\")\n",
    "            add_new_l = True\n",
    "\n",
    "        with open(out_file, \"a\") as file:\n",
    "            for datapoint in tqdm(self.filtered_dataset):\n",
    "                try:\n",
    "                    promt_generated_dict = self.generate_for_prompts(\n",
    "                        datapoint[\"text\"]\n",
    "                    )\n",
    "                    promt_generated_dict.update(datapoint)\n",
    "                    nline = \"\\n\" if add_new_l else \"\"\n",
    "\n",
    "                    file.write(f\"{nline}{json.dumps(promt_generated_dict)}\")\n",
    "                    add_new_l = True\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print(\n",
    "                        f\"Failed to get a response for ID: {datapoint['idx']}\"\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(\n",
    "                    ideology=\"liberal\",\n",
    "                    model_name=Generator._MODEL_CHATGPT_,\n",
    "                    trainingdata_profiling=True,\n",
    "                    use_fewshots=True,\n",
    "                    fewshots_num_examples=1,\n",
    "                    fewshots_w_semantic_similarity=False,\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.generate_for_prompts(\"Climate changes is not caused by humans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.generate_for_prompts(\"THIS IS AN INEFFECTIVE ARGUMENT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration notaphoenix--debateorg_w_effect_for_conservative-09f2d82db761569e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to /home/elba_ro/.cache/huggingface/datasets/notaphoenix___parquet/notaphoenix--debateorg_w_effect_for_conservative-09f2d82db761569e/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016559839248657227,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading data files",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13927f8592834de6b47059013c725d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004224538803100586,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Extracting data files",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3db501387b4ef7addc1c9a794369ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012681007385253906,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Generating test split",
       "rate": null,
       "total": 5740,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55942dcbee5b4d62aee75de0abb45ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5740 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004824161529541016,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Generating training split",
       "rate": null,
       "total": 40547,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a51bfef217014a9d912fbccea02e7ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating training split:   0%|          | 0/40547 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004677772521972656,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Generating validation split",
       "rate": null,
       "total": 11464,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3d427d88604a2f91fe789bed58bec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/11464 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/elba_ro/.cache/huggingface/datasets/notaphoenix___parquet/notaphoenix--debateorg_w_effect_for_conservative-09f2d82db761569e/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004266023635864258,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 6,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2822e673472844a78105aa62f37ba806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "found = load_dotenv(f\"../.env\")\n",
    "\n",
    "seed = 2062021\n",
    "name: str = f\"notaphoenix/debateorg_w_effect_for_conservative\"\n",
    "dataset: Dataset = load_dataset(name, split=\"test\")\n",
    "dataset = dataset.filter(\n",
    "    lambda x: x[\"label\"] == \"ineffective\"\n",
    ").shuffle(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'author', 'original_text', 'category', 'round', 'debate_id', 'idx'],\n",
       "    num_rows: 0\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iesta_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
