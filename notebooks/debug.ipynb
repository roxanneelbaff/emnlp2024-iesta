{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    LlamaTokenizer,\n",
    "    LlamaForCausalLM,\n",
    "    GenerationConfig,\n",
    "    pipeline,\n",
    ")\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = LlamaTokenizer.from_pretrained(\"chavinlo/alpaca-native\")\n",
    "found = load_dotenv(find_dotenv())\n",
    "print(f\"dotenv was {found}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "ALPACA_WEIGHTS_FOLDER = \"/localdata1/EmEx/model_weights/alpaca_7b\"\n",
    "\n",
    "alpaca_model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    ALPACA_WEIGHTS_FOLDER\n",
    ").to(device)\n",
    "alpaca_tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    ALPACA_WEIGHTS_FOLDER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=alpaca_model,\n",
    "    tokenizer=alpaca_tokenizer,\n",
    "    max_length=1024,\n",
    "    # temperature=0.2,\n",
    "    # top_p=0.95,\n",
    "    # repetition_penalty=1.2\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAIChat\n",
    "from langchain import LLMChain\n",
    "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "from ast import literal_eval\n",
    "import random\n",
    "import re\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "basic_str = \"Transform the following argument to an effective argument by maintaining the original length\"\n",
    "ideology_str = \"for readers with a {ideology} political ideology\"\n",
    "content_str = \"by preserving the content of the argument\"\n",
    "style_str = \"by only changing the style of the text\"\n",
    "\n",
    "prompt_dict = {\n",
    "    \"basic\": f\"{basic_str}:\",\n",
    "    \"ideology\": f\"{basic_str} {ideology_str}:\",\n",
    "    \"content\": f\"{basic_str} {content_str}:\",\n",
    "    \"style\": f\"{basic_str} {style_str}:\",\n",
    "    \"ideology-content\": f\"{basic_str} {ideology_str} {content_str}:\",\n",
    "    \"ideology-style\": f\"{basic_str} {ideology_str} {style_str}:\",\n",
    "    \"all\": f\"{basic_str} {ideology_str} {content_str} and {style_str}:\",\n",
    "}\n",
    "\n",
    "\n",
    "def create_prompt_template(prompt):\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(prompt)\n",
    "    human_template = \"{ineffective_argument}\"\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
    "        human_template\n",
    "    )\n",
    "    return [system_message_prompt, human_message_prompt]\n",
    "\n",
    "\n",
    "liberal_chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    create_prompt_template(prompt_dict[\"all\"].format(ideology=\"liberal\"))\n",
    ")\n",
    "llm_chain = LLMChain(llm=local_llm, prompt=liberal_chat_prompt)\n",
    "result = llm_chain.run(\n",
    "    ineffective_argument=\"If there was no Kryptonite, can Superman defeat the Silver Surfer?\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "ALPACA_WEIGHTS_FOLDER = \"/localdata1/EmEx/model_weights/alpaca_7b\"\n",
    "from transformers import (\n",
    "    LlamaForCausalLM,\n",
    "    LlamaTokenizer,\n",
    "    GenerationConfig,\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ALPACA_WEIGHTS_FOLDER)\n",
    "\n",
    "alpaca_llm = AutoModelForCausalLM.from_pretrained(\n",
    "    ALPACA_WEIGHTS_FOLDER,\n",
    "    # load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=alpaca_llm,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=1024,\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2,\n",
    ")\n",
    "local_model = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "template = \"\"\"Create an argument for Conservative readers related to a specific topic. Return at least 200 words.\n",
    "\n",
    "Topic:\n",
    "{topic}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"topic\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=local_model)\n",
    "qn = \"climate change\"\n",
    "print(llm_chain.run(qn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q install -i https://test.pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install cudatoolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall -y bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q install -U bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q uninstall -y bitsandbytes-cuda114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q uninstall -y bitsandbytes-cuda117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show iesta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import transformers\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from iesta.machine_learning.huggingface_loader import IESTAHuggingFace\n",
    "from ydata_profiling import ProfileReport\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from os.path import exists\n",
    "import dataclasses\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from datasets.combine import concatenate_datasets\n",
    "from iesta.machine_learning.huggingface_loader import IESTAHuggingFace\n",
    "from ydata_profiling import ProfileReport\n",
    "from langdetect import detect\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM \n",
    "from typing import ClassVar\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Generator:\n",
    "    ideology: str  # liberal or conservative\n",
    "    model_name: str  # gpt or alpaca\n",
    "\n",
    "    data_limit: int = 500\n",
    "    data_profiling: bool = False\n",
    "    data_save: bool = False\n",
    "    seed: int = 2062021\n",
    "\n",
    "    out_file: str = \"data/llms_out/\"\n",
    "\n",
    "    use_fewshots: bool = False\n",
    "    fewshots_num_examples: int = 1  # we use 1 to 3\n",
    "    fewshots_w_semantic_similarity: bool = False\n",
    "    verbose: int = 0\n",
    "    trainingdata_profiling: bool = True\n",
    "    _MODEL_CHATGPT_: ClassVar = \"gpt-3.5-turbo\"\n",
    "    _MODEL_ALPACA_: ClassVar = \"alpaca\"\n",
    "    _LIMIT_: ClassVar = 500\n",
    "\n",
    "    # HELPERS   #\n",
    "    # --------- #\n",
    "    @staticmethod\n",
    "    def init_prompts():\n",
    "        basic_str = \"Transform the following argument to an effective argument by maintaining the original length\"\n",
    "        ideology_str = \"for readers with a {ideology} political ideology\"\n",
    "        content_str = \"by preserving the content of the argument\"\n",
    "        style_str = \"by only changing the style of the text\"\n",
    "\n",
    "        prompt_dict = {\n",
    "            \"basic\": f\"{basic_str}:\",\n",
    "            \"ideology\": f\"{basic_str} {ideology_str}:\",\n",
    "            \"content\": f\"{basic_str} {content_str}:\",\n",
    "            \"style\": f\"{basic_str} {style_str}:\",\n",
    "            \"ideology-content\": f\"{basic_str} {ideology_str} {content_str}:\",\n",
    "            \"ideology-style\": f\"{basic_str} {ideology_str} {style_str}:\",\n",
    "            \"all\": f\"{basic_str} {ideology_str} {content_str} and {style_str}:\",\n",
    "        }\n",
    "        return prompt_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def create_prompt_template(prompt):\n",
    "        system_message_prompt = SystemMessagePromptTemplate.from_template(\n",
    "            prompt\n",
    "        )\n",
    "        human_template = \"Argument: {ineffective_argument}\"\n",
    "        human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
    "            human_template\n",
    "        )\n",
    "\n",
    "        return [system_message_prompt, human_message_prompt]\n",
    "\n",
    "    # ---- End Helpers --- #\n",
    "\n",
    "    def __post_init__(self):\n",
    "        found = load_dotenv(\"/home/elba_ro/repos/github/conf22-style-transfer/.env\")\n",
    "        print(f\"dotenv was found: {found}\")\n",
    "\n",
    "        print(\"Initializing all prompt templates in variable prompt_dict..\")\n",
    "        self.prompt_dict = Generator.init_prompts()\n",
    "\n",
    "        print(\n",
    "            f\"Initializing LLM for {self.model_name} in variable local_llm...\"\n",
    "        )\n",
    "        self.local_llm = self.get_model()\n",
    "\n",
    "        print(\n",
    "            f\"Getting filtered dataset top {self.data_limit} in variable\"\n",
    "            \"filtered_dataset...\"\n",
    "        )\n",
    "        self.filtered_dataset = self.get_data(effect=\"ineffective\")\n",
    "\n",
    "        if self.use_fewshots:\n",
    "            self.examples = self.get_examples()\n",
    "\n",
    "    def get_model(self):\n",
    "        if self.model_name == Generator._MODEL_CHATGPT_:\n",
    "            local_llm = ChatOpenAI(\n",
    "                model_name=Generator._MODEL_CHATGPT_, temperature=0\n",
    "            )\n",
    "        elif self.model_name == Generator._MODEL_ALPACA_:\n",
    "            # device = torch.device('cuda:0')\n",
    "            ALPACA_WEIGHTS_FOLDER = \"/localdata1/EmEx/model_weights/alpaca_7b\"\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained(ALPACA_WEIGHTS_FOLDER)\n",
    "            alpaca_llm = AutoModelForCausalLM.from_pretrained(\n",
    "                ALPACA_WEIGHTS_FOLDER,\n",
    "                # load_in_8bit=True,\n",
    "                device_map=\"cuda:1\",\n",
    "            )\n",
    "\n",
    "            pipe = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=alpaca_llm,\n",
    "                tokenizer=tokenizer,\n",
    "                max_length=4096,\n",
    "                temperature=0,\n",
    "                top_p=0.95,\n",
    "                repetition_penalty=1.2,\n",
    "            )\n",
    "            local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "        return local_llm\n",
    "\n",
    "    def get_data(self, effect=\"ineffective\"):\n",
    "        limit = Generator._LIMIT_\n",
    "        seed = 2062021\n",
    "        name: str = f\"notaphoenix/debateorg_w_effect_for_{self.ideology}\"\n",
    "        dataset: Dataset = load_dataset(name, split=\"test\")\n",
    "        dataset = dataset.filter(\n",
    "            lambda x: x[\"label\"] == IESTAHuggingFace._LABEL2ID_[effect]\n",
    "        ).shuffle(seed=seed)\n",
    "\n",
    "        if len(dataset) > limit:\n",
    "            dataset = dataset.select(range(limit))\n",
    "\n",
    "        print(f\"{len(dataset)} before len filter\")\n",
    "        dataset = dataset.filter(\n",
    "            lambda x: len(x[\"text\"].split(\" \")) > 10\n",
    "            and len(x[\"text\"].split(\" \")) <= 1024\n",
    "            and x[\"idx\"] != 64707\n",
    "            and detect(x[\"text\"]) == \"en\"\n",
    "        )\n",
    "        print(f\"{len(dataset)} after len filter\")\n",
    "\n",
    "        while len(dataset) < limit:\n",
    "            idxes = dataset.to_pandas()[\"idx\"].values.tolist()\n",
    "            dataset_extra: Dataset = load_dataset(name, split=\"test\")\n",
    "            dataset_extra = dataset_extra.filter(\n",
    "                lambda x: len(x[\"text\"].split(\" \")) > 10\n",
    "                and len(x[\"text\"].split(\" \")) <= 1024\n",
    "                and x[\"idx\"] != 64707\n",
    "                and detect(x[\"text\"]) == \"en\"\n",
    "            )\n",
    "\n",
    "            dataset_extra = dataset_extra.filter(\n",
    "                lambda x: x[\"label\"] == IESTAHuggingFace._LABEL2ID_[effect]\n",
    "                and [\"idx\"] not in idxes\n",
    "            ).shuffle(seed=seed)\n",
    "            dataset_extra = dataset_extra.select(range(limit - len(dataset)))\n",
    "            print(f\"{len(dataset_extra)} of extra\")\n",
    "            dataset = concatenate_datasets([dataset, dataset_extra])\n",
    "\n",
    "            print(f\"{len(dataset)} new length\")\n",
    "        print(f\"Return dataset {name} with {len(dataset)} \")\n",
    "        # dataset = dataset.map(lambda example, idx: {\"id\": idx, **example}, with_indices=True)\n",
    "\n",
    "        df = dataset.to_pandas().copy()\n",
    "        if self.data_profiling:\n",
    "            report = ProfileReport(df=df, minimal=False)\n",
    "            report.to_file(f\"{self.ideology}_test_{limit}_seed_{seed}\")\n",
    "\n",
    "        if self.data_save:\n",
    "            df.to_csv(f\"{self.ideology}_test_{limit}_seed_{seed}.csv\")\n",
    "        return dataset\n",
    "\n",
    "    def _run_test(self):\n",
    "        chat_prompt = ChatPromptTemplate.from_messages(\n",
    "            Generator.create_prompt_template(\n",
    "                self.prompt_dict[\"all\"].format(ideology=self.ideology)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        llm_chain = LLMChain(llm=self.local_llm, prompt=chat_prompt)\n",
    "        result = llm_chain.run(\n",
    "            ineffective_argument=\"Climate change \"\n",
    "            \"litigations are now linked to human rights. \"\n",
    "        )\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_examples(self, save: bool = True) -> list:\n",
    "        limit = Generator._LIMIT_ * self.fewshots_num_examples\n",
    "        seed = self.seed\n",
    "\n",
    "        name: str = f\"notaphoenix/debateorg_w_effect_for_{self.ideology}\"\n",
    "        dataset: Dataset = load_dataset(name, split=\"training\")\n",
    "        dataset = dataset.filter(\n",
    "            lambda x: x[\"label\"] == IESTAHuggingFace._LABEL2ID_[\"effective\"]\n",
    "            and len(x[\"text\"].split(\" \")) > 10\n",
    "            and len(x[\"text\"].split(\" \")) <= 1024\n",
    "            and detect(x[\"text\"]) == \"en\"\n",
    "        ).shuffle(seed=seed)\n",
    "\n",
    "        if len(dataset) > limit and not self.fewshots_w_semantic_similarity:\n",
    "            dataset = dataset.select(range(limit))\n",
    "        print(f\"{len(dataset)} new length\")\n",
    "        print(f\"Return dataset {name} with {len(dataset)} \")\n",
    "\n",
    "        df = dataset.to_pandas().copy()\n",
    "        filename: str = f\"{self.ideology}_training_{limit}_seed_{seed}_fewshot_{self.fewshots_num_examples}_similarity{self.fewshots_w_semantic_similarity}\"\n",
    "        if self.trainingdata_profiling:\n",
    "            report = ProfileReport(df=df, minimal=True)\n",
    "            report.to_file(f\"{filename}.html\")\n",
    "\n",
    "        df.to_csv(f\"{filename}.csv\")\n",
    "\n",
    "        result = [\n",
    "            {\"effective_argument\": x} for x in df[\"text\"].values.tolist()\n",
    "        ]\n",
    "        print(result[:3])\n",
    "        return result\n",
    "\n",
    "    # GENERATION #\n",
    "    # ---------- #\n",
    "    def generate_for_prompts(self, ineffective_argument: str):\n",
    "        result_dict = {}\n",
    "        print(\"generate_for_prompts called\")\n",
    "        # Preparing PROMPTS\n",
    "        local_examples = []\n",
    "        if self.use_fewshots:\n",
    "            if len(self.examples) < self.fewshots_num_examples:\n",
    "                print(\"warning: ran out of examples, replenishing...\") \n",
    "                self.examples = self.get_examples(save=False)\n",
    "\n",
    "            local_examples = [self.examples.pop() for _ in range(0, self.fewshots_num_examples)]\n",
    "\n",
    "        for k, prompt_template in self.prompt_dict.items():\n",
    "            if self.use_fewshots:\n",
    "                template = (\n",
    "                    prompt_template.format(ideology=self.ideology)\n",
    "                    + \"\\n    Argument: {ineffective_argument}\\n\"\n",
    "                )\n",
    "                # prompt = PromptTemplate(template=template, input_variables=[\"ineffective_argument\"])\n",
    "\n",
    "                example_prompt = PromptTemplate(\n",
    "                    input_variables=[\"effective_argument\"],\n",
    "                    template=\"An Example of an effective argument: {effective_argument}\",\n",
    "                )\n",
    "\n",
    "                prompt = FewShotPromptTemplate(\n",
    "                    examples=local_examples,\n",
    "                    example_prompt=example_prompt,\n",
    "                    suffix=template,\n",
    "                    input_variables=[\"ineffective_argument\"],\n",
    "                )\n",
    "            else:  # 0 shot\n",
    "                if self.model_name == Generator._MODEL_CHATGPT_:\n",
    "                    prompt = ChatPromptTemplate.from_messages(\n",
    "                        Generator.create_prompt_template(\n",
    "                            prompt_template.format(ideology=self.ideology)\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    template = (\n",
    "                        prompt_template.format(ideology=self.ideology)\n",
    "                        + \"\\n    Argument: {ineffective_argument}\\n    \"\n",
    "                    )\n",
    "                    prompt = PromptTemplate(\n",
    "                        template=template,\n",
    "                        input_variables=[\"ineffective_argument\"],\n",
    "                    )\n",
    "            print(\"****** prompt: \")\n",
    "            print(prompt.format(ineffective_argument=ineffective_argument))\n",
    "            llm_chain = LLMChain(llm=self.local_llm, prompt=prompt)\n",
    "            result_dict[k] = llm_chain.run(\n",
    "                ineffective_argument=ineffective_argument\n",
    "            )\n",
    "            result_dict[f\"len_{k}\"] = len(result_dict[k])\n",
    "            result_dict[\"len_orig\"] = len(ineffective_argument)\n",
    "            if self.use_fewshots:\n",
    "                result_dict[\"examples\"] = local_examples\n",
    "\n",
    "        return result_dict\n",
    "\n",
    "    def generate_all(self):\n",
    "        fewshots_text = (\n",
    "            f\"_{self.fewshots_num_examples}fewshots\"\n",
    "            if self.use_fewshots\n",
    "            else \"\"\n",
    "        )\n",
    "        fewshots_text = (\n",
    "            f\"{fewshots_text}_with_similarity\"\n",
    "            if self.fewshots_w_semantic_similarity\n",
    "            else fewshots_text\n",
    "        )\n",
    "        out_file = f\"{self.out_file}{self.ideology}_{self.model_name.lower()}{fewshots_text}.jsonl\"\n",
    "\n",
    "        existing_indices = []\n",
    "        if exists(out_file):\n",
    "            _df = pd.read_json(path_or_buf=out_file, lines=True)\n",
    "            existing_indices = _df[\"idx\"].values.tolist()\n",
    "\n",
    "        add_new_l = False\n",
    "        if len(existing_indices) > 0:\n",
    "            print(f\"filtering out existing indices ({len(existing_indices)})\")\n",
    "            self.filtered_dataset = self.filtered_dataset.filter(\n",
    "                lambda example: example[\"idx\"] not in existing_indices\n",
    "            )\n",
    "            print(f\"{self.filtered_dataset.num_rows} to go...\")\n",
    "            add_new_l = True\n",
    "\n",
    "        with open(out_file, \"a\") as file:\n",
    "            for datapoint in tqdm(self.filtered_dataset):\n",
    "                try:\n",
    "                    promt_generated_dict = self.generate_for_prompts(\n",
    "                        datapoint[\"text\"]\n",
    "                    )\n",
    "                    promt_generated_dict.update(datapoint)\n",
    "                    nline = \"\\n\" if add_new_l else \"\"\n",
    "\n",
    "                    file.write(f\"{nline}{json.dumps(promt_generated_dict)}\")\n",
    "                    add_new_l = True\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print(\n",
    "                        f\"Failed to get a response for ID: {datapoint['idx']}\"\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dotenv was found: True\n",
      "Initializing all prompt templates in variable prompt_dict..\n",
      "Initializing LLM for gpt-3.5-turbo in variable local_llm...\n",
      "Getting filtered dataset top 500 in variablefiltered_dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/elba_ro/.cache/huggingface/datasets/notaphoenix___parquet/notaphoenix--debateorg_w_effect_for_liberal-d0ad1689381f171f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e50e783f876344278983217ad96b5d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3886 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 before len filter\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90d953ed7f54acbb08f4363d940377f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "415 after len filter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/elba_ro/.cache/huggingface/datasets/notaphoenix___parquet/notaphoenix--debateorg_w_effect_for_liberal-d0ad1689381f171f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13f6114ecc14a93aea3faa6aa069e54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3886 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b0c3a3d2e141deb6a577f09284b172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3163 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 of extra\n",
      "500 new length\n",
      "Return dataset notaphoenix/debateorg_w_effect_for_liberal with 500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/elba_ro/.cache/huggingface/datasets/notaphoenix___parquet/notaphoenix--debateorg_w_effect_for_liberal-d0ad1689381f171f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "415bc154ed654f98be3e95e2fe8a6ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/27135 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 new length\n",
      "Return dataset notaphoenix/debateorg_w_effect_for_liberal with 500 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0970dc6809d4e62bcbc290bc21c53b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae1cf3e37ade4806a250b84657ec5ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cada9c97c04a450a82533be87a4635a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b454db3f6ba4fb48fd21154e4191a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'effective_argument': \"I accept, thanks for bringing up a new topic! I don't want my opening round to be very long, so I will bring up one point and let you present your opening arguments. Forcing people to vote (assuming if you don't, you are punished in some kind of way) is both immoral and can lead to donkey votes. The majority people who don't want to vote will vote to get it over with and not put any thought into it, giving the candidate an unfair advantage or disadvantage based really on luck, with no actual thought going into the vote. More candidates will be elected due to luck rather than what the general public really wants. Thanks, looking forward to a good argument!\"}, {'effective_argument': 'My opponent quotes: \"Okay, I am conceding defeat.\" My opponent has given up the debate and has accepted defeat. I heavily urge a Con vote. Thank you.'}, {'effective_argument': \"1 - In the example of ALFA corporation, Pro assumes that it would be beneficial for the corporation to utilize the other 75 acres of land. To use those acres, ALFA would have to incur expenses such as planting, watering, pesticides, consulting, harvesting, processing, and distribution of final product. These expenses are only offset, resulting in profit, if there is sufficient demand for the product. If there isn't enough demand, ALFA will incur a loss by using those extra acres. Why then is ALFA corporation only using 25 acres? I can only think of two reasons: There isn't enough demand to justify planting more crops, or ALFA corporation doesn't have the capital to carry the expenses through harvest. In the first case, taxing ALFA corporation won't do any good, in fact the tax would hurt a corporation that is already running below capacity due to poor economic conditions. In the second case, taxing ALFA corporation would only contribute to their capital problems, making future expansion more difficult. 2 - In the example of the two apartment complexes, there is obviously a reason for the second complex to have been built. Demand drives an economy, and the current demand trends in housing call for larger, open floor plans, modern appliances, modern fixtures, higher ceilings, and other such modern attributes to homes. Many people will choose to pay more to live in a newer, larger, modern apartment than a cheaper, older apartment. This demand is what causes new complexes to be built when others are still vacant. The only way to change the natural supply/demand process of the free market would be to take away the freedom and force people to move into existing complexes. Taxing the empty apartments would only hurt those who are trying to turn a profit in an area of housing where there is less demand. It would do nothing to incentivize people to consider housing that they don't desire. 3 - Pro states that increasing taxes on unused land and apartments would somehow stimulate the economy, without giving any reasons as to how such taxes would increase demand. Without demand, there can be no profit. I assert that unused apartments, homes, and land should not be taxed, because doing so only adds additional financial burden in an economy with low demand and struggling balance sheets.\"}]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "generate_for_prompts() missing 1 required positional argument: 'ineffective_argument'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/elba_ro/repos/github/conf22-style-transfer/notebooks/debug.ipynb Cell 18\u001b[0m in \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bteamserver/home/elba_ro/repos/github/conf22-style-transfer/notebooks/debug.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m generator \u001b[39m=\u001b[39m Generator(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bteamserver/home/elba_ro/repos/github/conf22-style-transfer/notebooks/debug.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m                     ideology\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mliberal\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bteamserver/home/elba_ro/repos/github/conf22-style-transfer/notebooks/debug.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m                     model_name\u001b[39m=\u001b[39mGenerator\u001b[39m.\u001b[39m_MODEL_CHATGPT_,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bteamserver/home/elba_ro/repos/github/conf22-style-transfer/notebooks/debug.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m                     fewshots_w_semantic_similarity\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bteamserver/home/elba_ro/repos/github/conf22-style-transfer/notebooks/debug.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m                 )\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bteamserver/home/elba_ro/repos/github/conf22-style-transfer/notebooks/debug.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m generator\u001b[39m.\u001b[39;49mgenerate_for_prompts()\n",
      "\u001b[0;31mTypeError\u001b[0m: generate_for_prompts() missing 1 required positional argument: 'ineffective_argument'"
     ]
    }
   ],
   "source": [
    "generator = Generator(\n",
    "                    ideology=\"liberal\",\n",
    "                    model_name=Generator._MODEL_CHATGPT_,\n",
    "                    trainingdata_profiling=True,\n",
    "                    use_fewshots=True,\n",
    "                    fewshots_num_examples=1,\n",
    "                    fewshots_w_semantic_similarity=False,\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iesta_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
