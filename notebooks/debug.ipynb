{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "\n",
    "    pipeline,\n",
    ")\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = LlamaTokenizer.from_pretrained(\"chavinlo/alpaca-native\")\n",
    "found = load_dotenv(find_dotenv())\n",
    "print(f\"dotenv was {found}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "ALPACA_WEIGHTS_FOLDER = \"/localdata1/EmEx/model_weights/alpaca_7b\"\n",
    "\n",
    "alpaca_model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    ALPACA_WEIGHTS_FOLDER\n",
    ").to(device)\n",
    "alpaca_tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    ALPACA_WEIGHTS_FOLDER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=alpaca_model,\n",
    "    tokenizer=alpaca_tokenizer,\n",
    "    max_length=1024,\n",
    "    # temperature=0.2,\n",
    "    # top_p=0.95,\n",
    "    # repetition_penalty=1.2\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAIChat\n",
    "from langchain import LLMChain\n",
    "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "from ast import literal_eval\n",
    "import random\n",
    "import re\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "basic_str = \"Transform the following argument to an effective argument by maintaining the original length\"\n",
    "ideology_str = \"for readers with a {ideology} political ideology\"\n",
    "content_str = \"by preserving the content of the argument\"\n",
    "style_str = \"by only changing the style of the text\"\n",
    "\n",
    "prompt_dict = {\n",
    "    \"basic\": f\"{basic_str}:\",\n",
    "    \"ideology\": f\"{basic_str} {ideology_str}:\",\n",
    "    \"content\": f\"{basic_str} {content_str}:\",\n",
    "    \"style\": f\"{basic_str} {style_str}:\",\n",
    "    \"ideology-content\": f\"{basic_str} {ideology_str} {content_str}:\",\n",
    "    \"ideology-style\": f\"{basic_str} {ideology_str} {style_str}:\",\n",
    "    \"all\": f\"{basic_str} {ideology_str} {content_str} and {style_str}:\",\n",
    "}\n",
    "\n",
    "\n",
    "def create_prompt_template(prompt):\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(prompt)\n",
    "    human_template = \"{ineffective_argument}\"\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
    "        human_template\n",
    "    )\n",
    "    return [system_message_prompt, human_message_prompt]\n",
    "\n",
    "\n",
    "liberal_chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    create_prompt_template(prompt_dict[\"all\"].format(ideology=\"liberal\"))\n",
    ")\n",
    "llm_chain = LLMChain(llm=local_llm, prompt=liberal_chat_prompt)\n",
    "result = llm_chain.run(\n",
    "    ineffective_argument=\"If there was no Kryptonite, can Superman defeat the Silver Surfer?\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "ALPACA_WEIGHTS_FOLDER = \"/localdata1/EmEx/model_weights/alpaca_7b\"\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(ALPACA_WEIGHTS_FOLDER)\n",
    "'''\n",
    "alpaca_llm = AutoModelForCausalLM.from_pretrained(\n",
    "    ALPACA_WEIGHTS_FOLDER,\n",
    "    # load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=alpaca_llm,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=1024,\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2,\n",
    ")\n",
    "local_model = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "template = \"\"\"Create an argument for Conservative readers related to a specific topic. Return at least 200 words.\n",
    "\n",
    "Topic:\n",
    "{topic}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"topic\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=local_model)\n",
    "qn = \"climate change\"\n",
    "print(llm_chain.run(qn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q install -i https://test.pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install cudatoolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall -y bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q install -U bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q uninstall -y bitsandbytes-cuda114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q uninstall -y bitsandbytes-cuda117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show iesta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from iesta.data.huggingface_loader import IESTAHuggingFace\n",
    "from ydata_profiling import ProfileReport\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from os.path import exists\n",
    "import dataclasses\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from datasets.combine import concatenate_datasets\n",
    "from iesta.data.huggingface_loader import IESTAHuggingFace\n",
    "from ydata_profiling import ProfileReport\n",
    "from langdetect import detect\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM \n",
    "from typing import ClassVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(\n",
    "                    ideology=\"liberal\",\n",
    "                    model_name=Generator._MODEL_CHATGPT_,\n",
    "                    use_fewshots=True,\n",
    "                    n_shots=1,\n",
    "                    fewshots_w_semantic_similarity=False,\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.generate_for_prompts(\"Climate changes is not caused by humans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dotenv was True\n",
      "loading model gpt-3.5-turbo\n"
     ]
    }
   ],
   "source": [
    "from iesta.llms.models import IestaLLM, ChatGpt\n",
    "llm_model: IestaLLM \n",
    "x = ChatGpt()\n",
    "llm_model = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip -q install ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** prompt WITHIN input_variables=[] output_parser=None partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template='syste,', template_format='f-string', validate_template=True), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template='inst', template_format='f-string', validate_template=True), additional_kwargs={})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=[], output_parser=None, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template='syste,', template_format='f-string', validate_template=True), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template='inst', template_format='f-string', validate_template=True), additional_kwargs={})])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.get_prompt_template(\"inst\", \"syste,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "found = load_dotenv(f\"../.env\")\n",
    "\n",
    "seed = 2062021\n",
    "name: str = f\"notaphoenix/debateorg_w_effect_for_conservative\"\n",
    "dataset: Dataset = load_dataset(name, split=\"test\")\n",
    "dataset = dataset.filter(\n",
    "    lambda x: x[\"label\"] == 0\n",
    ").shuffle(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts.loc[\"Politics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = test_data.to_pandas()[\"category\"].value_counts().to_frame('counts')\n",
    "for category, count_row in category_counts.iterrows():\n",
    "    print(f\"{category}: {count_row['counts']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(effect=\"ineffective\", ideology=\"liberal\"):\n",
    "        limit = 500\n",
    "        seed = 2062021\n",
    "        name: str = f\"notaphoenix/debateorg_w_effect_for_{ideology}\"\n",
    "        dataset: Dataset = load_dataset(name, split=\"test\")\n",
    "        dataset = dataset.filter(\n",
    "            lambda x: x[\"label\"] == IESTAHuggingFace._LABEL2ID_[effect]\n",
    "        ).shuffle(seed=seed)\n",
    "\n",
    "        if len(dataset) > limit:\n",
    "            dataset = dataset.select(range(limit))\n",
    "\n",
    "        print(f\"{len(dataset)} before len filter\")\n",
    "        dataset = dataset.filter(\n",
    "            lambda x: len(x[\"text\"].split(\" \")) > 10\n",
    "            and len(x[\"text\"].split(\" \")) <= 1024\n",
    "            and x[\"idx\"] != 64707\n",
    "            and detect(x[\"text\"]) == \"en\"\n",
    "        )\n",
    "        print(f\"{len(dataset)} after len filter\")\n",
    "\n",
    "        while len(dataset) < limit:\n",
    "            idxes = dataset.to_pandas()[\"idx\"].values.tolist()\n",
    "            dataset_extra: Dataset = load_dataset(name, split=\"test\")\n",
    "            dataset_extra = dataset_extra.filter(\n",
    "                lambda x: len(x[\"text\"].split(\" \")) > 10\n",
    "                and len(x[\"text\"].split(\" \")) <= 1024\n",
    "                and x[\"idx\"] != 64707\n",
    "                and detect(x[\"text\"]) == \"en\"\n",
    "            )\n",
    "\n",
    "            dataset_extra = dataset_extra.filter(\n",
    "                lambda x: x[\"label\"] == IESTAHuggingFace._LABEL2ID_[effect]\n",
    "                and [\"idx\"] not in idxes\n",
    "            ).shuffle(seed=seed)\n",
    "            dataset_extra = dataset_extra.select(range(limit - len(dataset)))\n",
    "            print(f\"{len(dataset_extra)} of extra\")\n",
    "            dataset = concatenate_datasets([dataset, dataset_extra])\n",
    "\n",
    "            print(f\"{len(dataset)} new length\")\n",
    "        print(f\"Return dataset {name} with {len(dataset)} \")\n",
    "        # dataset = dataset.map(lambda example, idx: {\"id\": idx, **example}, with_indices=True)\n",
    "\n",
    "        df = dataset.to_pandas().copy()\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iesta_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
