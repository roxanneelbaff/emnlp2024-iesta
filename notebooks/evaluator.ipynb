{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "import torch\n",
    "def set_css():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pip show iesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iesta.llms.generate import Generator\n",
    "import argparse\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "import iesta\n",
    "import iesta.llms\n",
    "import iesta.llms.models\n",
    "from iesta.llms.models import LlamaV2, ChatGpt\n",
    "\n",
    "import importlib\n",
    "importlib.reload(iesta)\n",
    "importlib.reload(iesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -q install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_experiments = _fetch_all_experiments()\n",
    "exceptions_dict = {}# {k:[] for k in all_experiments}\n",
    "\n",
    "exceptions_dict[\"liberal_chatgpt_0shot\"] = [44708, 35204, 3176, 42440, 34248, 51598, 45614, 38169, 16602, 3259, 252,]\n",
    "exceptions_dict[\"conservative_chatgpt_0shot\"] = [79903, 17012, 66698, 824, 43599, 46682, 78779, 78643, 58514, 43596, 25775, 32238, 61381]\n",
    "exceptions_dict[\"liberal_chatgpt_1shot\"] = [252, 20310, 18397, 9161, 45614]\n",
    "exceptions_dict[\"conservative_chatgpt_1shot\"] = [79903, 66698, 40673, 23110, 13417, 46682, 78778, 78643, 11374, 43596, 9181, 49307, 48861,] # 10968 \"Vote for me for bacon and strippers\"\n",
    "\n",
    "total = 0\n",
    "for k,v in exceptions_dict.items():\n",
    "    print(f\"{k}; {len(v)}\")\n",
    "    total = total+ len(v)\n",
    "\n",
    "total\n",
    "total*100/14000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = processed_toxicity_dict['liberal_llamav2_0shot_steered_medianl0.2_processed.csv']\n",
    "# sns.countplot(data=df, x=\"success\", hue=\"toxic_str\")\n",
    "counter_arr = []\n",
    "for k, experiment in processed_toxicity_dict.items():\n",
    "    # exp_res = {\"experiment\": experiment}\n",
    "    \n",
    "    exp_res =experiment[[\"success\", \"toxic_str\"]].groupby([\"success\", \"toxic_str\"])[\"toxic_str\"].agg([ \"count\"]).add_prefix('toxicity_').reset_index()\n",
    "    exp_res[\"experiment\"] = k.replace(\".csv\", \"\")\n",
    "    exp_res[\"ideology\"] = k.split(\"_\")[0]\n",
    "    exp_res[\"model\"] = k.split(\"_\")[1]\n",
    "    exp_res[\"Shot\"] = k.split(\"_\")[2]\n",
    "    \n",
    "    counter_arr.append(exp_res)\n",
    "\n",
    "\n",
    "def _apply_format(row):\n",
    "    row[\"success_toxic\"] = f\"{'Response' if row['success'] else 'No Response'} w {row['toxic_str'].replace('Not ', 'Non-')} Argument\"\n",
    "    return row\n",
    "concat_count_df = pd.concat(counter_arr, ignore_index=True, )\n",
    "concat_count_df = concat_count_df.apply(_apply_format, axis=1)\n",
    "concat_count_df = concat_count_df[[\"success_toxic\", \"toxicity_count\", \"experiment\", \"model\"]]\n",
    "concat_count_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat_count_df.set_index(\"experiment\")\n",
    "ax = sns.barplot(concat_count_df, x=\"experiment\", y=\"toxicity_count\", hue=\"success_toxic\", dodge=True)#, col=\"model\")\n",
    "#ax.plot(1955, 3600, \"*\", markersize=10, color=\"r\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q install  ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show iesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import iesta\n",
    "import iesta.evaluator\n",
    "importlib.reload(iesta)\n",
    "\n",
    "importlib.reload(iesta.evaluator)\n",
    "from iesta.evaluator import generation_processor, evaluator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iesta.llms.generate import Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = generation_processor._fetch_all_experiments(root=\"\")\n",
    "experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_eval_dict = {}\n",
    "for experiment in experiments:\n",
    "    if experiment in [\"liberal_llamav2_13b_0shot\" ]:\n",
    "        continue\n",
    "    splits = experiment.split(\"_\")\n",
    "    ideology = splits[0]\n",
    "    model_type = splits[1]\n",
    "    # if model_type == \"chatgpt\": continue\n",
    "    shot = splits[2].replace(\"shot\", \"\")\n",
    "    steered = f\"_{splits[3]}_{splits[4]}\" if len(splits) >4 else \"\"\n",
    "    experiment_eval_dict[experiment] = evaluator.Evaluator(model_type=model_type,\n",
    "                                                           ideology=ideology, \n",
    "                                                           shot_num=int(shot),\n",
    "                                                           steered=steered,\n",
    "                                                           root_path=\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conservative_chatgpt_0shot\n",
      "conservative_llamav2_0shot\n",
      "conservative_llamav2_0shot_steered_meanl0.2\n",
      "conservative_llamav2_0shot_steered_meanl0.5\n",
      "conservative_llamav2_0shot_steered_medianl0.2\n",
      "conservative_llamav2_1shot\n",
      "liberal_chatgpt_0shot\n",
      "liberal_chatgpt_1shot\n",
      "liberal_llamav2_0shot\n",
      "liberal_llamav2_0shot_steered_meanl0.2\n",
      "liberal_llamav2_0shot_steered_meanl0.5\n",
      "liberal_llamav2_0shot_steered_medianl0.2\n",
      "liberal_llamav2_1shot\n"
     ]
    }
   ],
   "source": [
    "#experiment_eval_dict = {}\n",
    "for experiment in experiments:\n",
    "    if len(experiment_eval_dict[experiment].merged_df) == 3500:\n",
    "        print(experiment)\n",
    "        \n",
    "    if experiment == \"conservative_llamav2_1shot\":\n",
    "        experiment_eval_dict[experiment] = experiment_eval_dict[experiment].score_style()\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "corr_prompt_dict = {\"liberal\": [], \"conservative\": []}\n",
    "for key, evaluator_exp in experiment_eval_dict.items():\n",
    "    ideology = evaluator_exp.ideology\n",
    "    model_type = evaluator_exp.model_type\n",
    "    if model_type !=\"llamav2\":\n",
    "        continue\n",
    "    \n",
    "    corr_df = evaluator_exp.corr_toxicity_no_response\n",
    "    corr_df =corr_df[corr_df[\"group_type\"] == \"\"]\n",
    "    corr_df = corr_df.drop('group_value', axis=1)\n",
    "    corr_df = corr_df.drop('group_type', axis=1)\n",
    "    def remove_ide(row):\n",
    "        row['experiment'] = row['experiment'].replace(f\"{ideology}_\", \"\")\n",
    "        return row\n",
    "    corr_df = corr_df.apply(remove_ide, axis=1)\n",
    "    corr_df = corr_df.set_index(\"experiment\")\n",
    "    corr_df = corr_df.add_suffix(f'_{ideology}')\n",
    "\n",
    "    corr_prompt_dict[ideology].append(corr_df)\n",
    "\n",
    "liberal = pd.concat(corr_prompt_dict[\"liberal\"], join='outer')\n",
    "conservative = pd.concat(corr_prompt_dict[\"conservative\"])\n",
    "all_corr_df = pd.concat([conservative, liberal], axis=1)\n",
    "all_corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_extended_counts(experiment_eval_dict, count_type):\n",
    "\n",
    "    count_prompt_dict = {\"liberal\": [], \"conservative\": []}\n",
    "    for key, evaluator_exp in experiment_eval_dict.items():\n",
    "        ideology = evaluator_exp.ideology\n",
    "        model_type = evaluator_exp.model_type\n",
    "        if model_type !=\"llamav2\":\n",
    "            continue\n",
    "        if count_type == \"toxicity\":\n",
    "            count_df = evaluator_exp.count_success_per_toxic_df\n",
    "        elif count_type == \"prompt\":\n",
    "            count_df = evaluator_exp.count_success_per_prompt_df\n",
    "        else:\n",
    "            count_df = evaluator_exp.count_success_per_ideology_prompt_df\n",
    "        count_prompt_df = count_df[[\"fail_perc\"]].rename(columns= {\"fail_perc\": key.replace(f\"{ideology}_\", \"\")}).T\n",
    "        count_prompt_df = count_prompt_df.drop('All', axis=1)\n",
    "        count_prompt_df = count_prompt_df.add_suffix(f'_{ideology}')\n",
    "        count_prompt_df.score_style()\n",
    "        count_prompt_dict[ideology].append(count_prompt_df)\n",
    "        break\n",
    "\n",
    "    liberal = pd.concat(count_prompt_dict[\"liberal\"])\n",
    "    conservative = pd.concat(count_prompt_dict[\"conservative\"])\n",
    "    all_prompt_extended_df = pd.concat([conservative, liberal], axis=1)\n",
    "    return all_prompt_extended_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideology_prompt_count = get_extended_counts(experiment_eval_dict, \"ideology_prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_prompt_count = get_extended_counts(experiment_eval_dict, \"toxicity\")#.to_csv(\"data/out/toxicity_success_extended.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_extended_df = pd.concat([toxicity_prompt_count, ideology_prompt_count], axis=1)\n",
    "count_extended_df[['Toxic_conservative', 'Not Toxic_conservative', 'True_conservative', 'False_conservative',\n",
    "                   'Toxic_liberal', 'Not Toxic_liberal', 'True_liberal', 'False_liberal']].to_csv(\"data/out/failed_to_respond_perc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
