{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If kernel is not detected: F1 -> Developer: Reload Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAIChat\n",
    "from langchain import LLMChain\n",
    "from langchain.text_splitter import CharacterTextSplitter,TokenTextSplitter\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "from ast import literal_eval\n",
    "import random\n",
    "import re\n",
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source ../../../elbaff_iesta_venv/bin/activate\n",
    "# %pip install langchain\n",
    "# %pip install python-dotenv\n",
    "#%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nliberal_chat_prompt = ChatPromptTemplate.from_messages(create_prompt_template(prompt_dict[\"all\"].format(ideology =\"liberal\")))\\nllm_chain = LLMChain(llm=chat, prompt=liberal_chat_prompt)\\nresult = llm_chain.run(ineffective_argument=\"If there was no Kryptonite, can Superman defeat the Silver Surfer?\")\\nprint(result)\\ncons_chat_prompt = ChatPromptTemplate.from_messages(create_prompt_template(prompt_dict[\"all\"].format(ideology =\"conservative\")))\\nllm_chain = LLMChain(llm=chat, prompt=cons_chat_prompt)\\nresult = llm_chain.run(ineffective_argument=\"If there was no Kryptonite, can Superman defeat the Silver Surfer?\")\\nprint(result)\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "liberal_chat_prompt = ChatPromptTemplate.from_messages(create_prompt_template(prompt_dict[\"all\"].format(ideology =\"liberal\")))\n",
    "llm_chain = LLMChain(llm=chat, prompt=liberal_chat_prompt)\n",
    "result = llm_chain.run(ineffective_argument=\"If there was no Kryptonite, can Superman defeat the Silver Surfer?\")\n",
    "print(result)\n",
    "cons_chat_prompt = ChatPromptTemplate.from_messages(create_prompt_template(prompt_dict[\"all\"].format(ideology =\"conservative\")))\n",
    "llm_chain = LLMChain(llm=chat, prompt=cons_chat_prompt)\n",
    "result = llm_chain.run(ineffective_argument=\"If there was no Kryptonite, can Superman defeat the Silver Surfer?\")\n",
    "print(result)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new OpenAI instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_prompt_template(prompt):\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(prompt)\n",
    "    human_template=\"{ineffective_argument}\"\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "    return [system_message_prompt, human_message_prompt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from iesta.machine_learning.huggingface_loader import IESTAHuggingFace\n",
    "\n",
    "def get_data(ideology, effect='ineffective', limit=500):\n",
    "    name:str = f'notaphoenix/debateorg_w_effect_for_{ideology}'\n",
    "    dataset: Dataset = load_dataset(name, split=\"test\")\n",
    "    dataset = dataset.filter(lambda x: x[\"label\"] == IESTAHuggingFace._LABEL2ID_[effect]).shuffle(seed=2062021)\n",
    "    \n",
    "    if len(dataset) > limit:\n",
    "        dataset = dataset.select(range(limit))\n",
    "    print(f\"Return dataset {name} with {len(dataset)} \")\n",
    "    dataset = dataset.map(lambda example, idx: {\"id\": idx, **example}, with_indices=True)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "found = load_dotenv(find_dotenv())\n",
    "\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_model(model_name):\n",
    "    if model_name == \"chatgpt\":\n",
    "        return ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "    elif model_name == \"falcon\":\n",
    "        return HuggingFaceHub(huggingfacehub_api_token=os.getenv('HUGGINGFACE_TOKEN'),\n",
    "                              repo_id=\"tiiuae/falcon-7b-instruct\", \n",
    "                              model_kwargs={\"temperature\":0, \"max_new_tokens\":1000})\n",
    "\n",
    "chat = get_model(\"falcon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_generations(ineffective_argument:str, ideology:str, llm:str=\"chatgpt\"):\n",
    "    \n",
    "    result_dict = {}\n",
    "    for k, prompt_template in prompt_dict.items():\n",
    "        chat_prompt = ChatPromptTemplate.from_messages(create_prompt_template(prompt_template.format(ideology=ideology)))\n",
    "        llm_chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "        result_dict[k] = llm_chain.run(ineffective_argument=ineffective_argument)\n",
    "        #print(f\"'{result}'\\n\\n\")\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from os.path import exists\n",
    "\n",
    "def generate_args(ideology:str, out_file :str = \"../data/llms_out/\") -> pd.DataFrame:\n",
    "    out_file = f\"{out_file}{ideology}_gpt3.5turbo.jsonl\"\n",
    "    \n",
    "    existing_indices = []\n",
    "    \n",
    "    if exists(out_file):\n",
    "        _df = pd.read_json(path_or_buf=out_file, lines=True)\n",
    "        existing_indices = _df['id'].values.tolist()\n",
    "    \n",
    "    filtered_dataset = get_data(ideology, effect=\"ineffective\", limit=500)\n",
    "    add_new_l = False\n",
    "    if len(existing_indices) > 0 :\n",
    "        print(f\"filtering out existing indices ({len(existing_indices)})\")\n",
    "        filtered_dataset = filtered_dataset.filter(lambda example: example['id'] not in existing_indices)\n",
    "        print(f\"{filtered_dataset.num_rows} to go...\")\n",
    "        add_new_l = True\n",
    "    \n",
    "    with open(out_file, 'a') as file:\n",
    "\n",
    "        for datapoint in tqdm(filtered_dataset):\n",
    "            try:    \n",
    "                \n",
    "                promt_generated_dict = get_generations(datapoint[\"text\"], ideology)\n",
    "                promt_generated_dict.update(datapoint)\n",
    "\n",
    "                nline = \"\\n\" if add_new_l else \"\"\n",
    "\n",
    "                file.write(f\"{nline}{json.dumps(promt_generated_dict)}\")\n",
    "                add_new_l = True\n",
    "            except Exception as e:\n",
    "\n",
    "                print(e)\n",
    "                print(f\"Failed to get a response for ID: {datapoint['id']}\")   \n",
    "\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_args(ideology=\"liberal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_args(ideology=\"conservative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dataset(\"notaphoenix/debateorg_w_effect_for_liberal\", split=\"test\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in result_dict.items():\n",
    "    print(f\"\\n{k} - {prompt_dict[k]}\")\n",
    "    diff = Redlines(ineffective_argument,v)\n",
    "    display(Markdown(diff.output_markdown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "for ptype1, ptype2 in itertools.combinations_with_replacement(prompt_dict.keys(), 2):\n",
    "    if ptype1 == ptype2:\n",
    "        continue\n",
    "    print(f\"\\n{ptype1} VS. {ptype2}\")\n",
    "    print(len(f\"{ptype1} VS. {ptype2}\")*\"-\")\n",
    "\n",
    "    diff = Redlines(result_dict[ptype1],result_dict[ptype2])\n",
    "    display(Markdown(diff.output_markdown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in result_dict.items():\n",
    "    for k,v  in result_dict.items():\n",
    "    print(f\"\\n{k} - {prompt_dict[k]}\")\n",
    "    diff = Redlines(ineffective_argument,v)\n",
    "    display(Markdown(diff.output_markdown))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iesta_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
