{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show iesta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iesta.llms.generate import Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(\n",
    "    ideology=\"liberal\",\n",
    "    model_name=Generator._MODEL_ALPACA_,\n",
    "    out_file=\"../data/llms_out/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ineffective_argument = generator.filtered_dataset[123][\"text\"]\n",
    "ineffective_argument[:100]\n",
    "generator.generate_for_prompts(ineffective_argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_prompts():\n",
    "    basic_str = \"Transform the following argument to an effective argument by maintaining the original length\"\n",
    "    ideology_str = \"for readers with a {ideology} political ideology\"\n",
    "    content_str = \"by preserving the content of the argument\"\n",
    "    style_str = \"by only changing the style of the text\"\n",
    "\n",
    "    prompt_dict = {\n",
    "        \"basic\": f\"{basic_str}:\",\n",
    "        \"ideology\": f\"{basic_str} {ideology_str}:\",\n",
    "        \"content\": f\"{basic_str} {content_str}:\",\n",
    "        \"style\": f\"{basic_str} {style_str}:\",\n",
    "        \"ideology-content\": f\"{basic_str} {ideology_str} {content_str}:\",\n",
    "        \"ideology-style\": f\"{basic_str} {ideology_str} {style_str}:\",\n",
    "        \"all\": f\"{basic_str} {ideology_str} {content_str} and {style_str}:\",\n",
    "    }\n",
    "    return prompt_dict\n",
    "\n",
    "\n",
    "def create_prompt_template(prompt):\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(prompt)\n",
    "    human_template = \"Argument: {ineffective_argument}\"\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
    "        human_template\n",
    "    )\n",
    "\n",
    "    return [system_message_prompt, human_message_prompt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "\n",
    "template = \"\"\"Transform the following argument to an effective argument by maintaining the original length:\n",
    "    Argument: {ineffective_argument}\n",
    "    \"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template, input_variables=[\"ineffective_argument\"]\n",
    ")\n",
    "\n",
    "\n",
    "llm_chain = LLMChain(llm=generator.local_llm, prompt=prompt)\n",
    "llm_chain.run(ineffective_argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show iesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pip install -q transformers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U srsly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q langchain \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q xformers\n",
    "pip show ydata-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAIChat\n",
    "from langchain import LLMChain\n",
    "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "from ast import literal_eval\n",
    "import random\n",
    "import re\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !source ../../../elbaff_iesta_venv/bin/activate\n",
    "# %pip install langchain\n",
    "# %pip install python-dotenv\n",
    "# %pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "liberal_chat_prompt = ChatPromptTemplate.from_messages(create_prompt_template(prompt_dict[\"all\"].format(ideology =\"liberal\")))\n",
    "llm_chain = LLMChain(llm=chat, prompt=liberal_chat_prompt)\n",
    "result = llm_chain.run(ineffective_argument=\"If there was no Kryptonite, can Superman defeat the Silver Surfer?\")\n",
    "print(result)\n",
    "cons_chat_prompt = ChatPromptTemplate.from_messages(create_prompt_template(prompt_dict[\"all\"].format(ideology =\"conservative\")))\n",
    "llm_chain = LLMChain(llm=chat, prompt=cons_chat_prompt)\n",
    "result = llm_chain.run(ineffective_argument=\"If there was no Kryptonite, can Superman defeat the Silver Surfer?\")\n",
    "print(result)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "class FalconInit:\n",
    "    def __init__(self):\n",
    "        self.device = (\n",
    "            f\"cuda:{cuda.current_device()}\" if cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "\n",
    "        # creating a model\n",
    "        self.fmodel = AutoModelForCausalLM.from_pretrained(\n",
    "            \"tiiuae/falcon-7b-instruct\",\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=bfloat16,\n",
    "        )\n",
    "        self.fmodel.eval()\n",
    "        self.fmodel.to(self.device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"tiiuae/falcon-7b-instruct\"\n",
    "        )\n",
    "\n",
    "        self.text_generation_pipeline = transformers.pipeline(\n",
    "            model=self.fmodel,\n",
    "            tokenizer=self.tokenizer,\n",
    "            task=\"text-generation\",\n",
    "            return_full_text=True,\n",
    "            device=self.device,\n",
    "            max_length=10000,\n",
    "            temperature=0.1,\n",
    "            top_p=0.15,  # select from top tokens whose probability adds up to 15%\n",
    "            top_k=0,  # selecting from top 0 tokens\n",
    "            repetition_penalty=1.1,  # without a penalty, output starts to repeat\n",
    "            do_sample=True,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "found = load_dotenv(find_dotenv())\n",
    "\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "\n",
    "# Create a new OpenAI instance\n",
    "def get_model(model_name):\n",
    "    if model_name == \"chatgpt\":\n",
    "        return ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "    elif model_name == \"falcon\":\n",
    "        return HuggingFaceHub(\n",
    "            huggingfacehub_api_token=os.getenv(\"HUGGINGFACE_TOKEN\"),\n",
    "            repo_id=\"tiiuae/falcon-7b-instruct\",\n",
    "            model_kwargs={\"temperature\": 0.6, \"max_new_tokens\": 1000},\n",
    "        )\n",
    "\n",
    "\n",
    "chat = get_model(\"chatgpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_str = \"Transform the following argument to an effective argument by maintaining the original length\"\n",
    "ideology_str = \"for readers with a {ideology} political ideology\"\n",
    "content_str = \"by preserving the content of the argument\"\n",
    "style_str = \"by only changing the style of the text\"\n",
    "\n",
    "prompt_dict = {\n",
    "    \"basic\": f\"{basic_str}:\",\n",
    "    \"ideology\": f\"{basic_str} {ideology_str}:\",\n",
    "    \"content\": f\"{basic_str} {content_str}:\",\n",
    "    \"style\": f\"{basic_str} {style_str}:\",\n",
    "    \"ideology-content\": f\"{basic_str} {ideology_str} {content_str}:\",\n",
    "    \"ideology-style\": f\"{basic_str} {ideology_str} {style_str}:\",\n",
    "    \"all\": f\"{basic_str} {ideology_str} {content_str} and {style_str}:\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prompt_dict[\"all\"].split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_template(prompt):\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(prompt)\n",
    "    human_template = \"{ineffective_argument}\"\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
    "        human_template\n",
    "    )\n",
    "    return [system_message_prompt, human_message_prompt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip -q install langdetect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from datasets.combine import concatenate_datasets\n",
    "from iesta.data.huggingface_loader import IESTAHuggingFace\n",
    "from ydata_profiling import ProfileReport\n",
    "from langdetect import detect\n",
    "\n",
    "_MAX_LMIT = 4061  # 4097 - 36 the longest prompt\n",
    "\n",
    "\n",
    "def get_data(\n",
    "    ideology,\n",
    "    effect=\"ineffective\",\n",
    "    limit=500,\n",
    "    profile: bool = False,\n",
    "    save: bool = True,\n",
    "):\n",
    "    seed = 2062021\n",
    "    name: str = f\"notaphoenix/debateorg_w_effect_for_{ideology}\"\n",
    "    dataset: Dataset = load_dataset(name, split=\"test\")\n",
    "    dataset = dataset.filter(\n",
    "        lambda x: x[\"label\"] == IESTAHuggingFace._LABEL2ID_[effect]\n",
    "    ).shuffle(seed=seed)\n",
    "\n",
    "    if len(dataset) > limit:\n",
    "        dataset = dataset.select(range(limit))\n",
    "\n",
    "    print(f\"{len(dataset)} before len filter\")\n",
    "    dataset = dataset.filter(\n",
    "        lambda x: len(x[\"text\"].split(\" \")) > 10\n",
    "        and len(x[\"text\"].split(\" \")) <= 1024\n",
    "        and x[\"idx\"] != 64707\n",
    "        and detect(x[\"text\"]) == \"en\"\n",
    "    )\n",
    "    print(f\"{len(dataset)} after len filter\")\n",
    "\n",
    "    while len(dataset) < limit:\n",
    "        idxes = dataset.to_pandas()[\"idx\"].values.tolist()\n",
    "        dataset_extra: Dataset = load_dataset(name, split=\"test\")\n",
    "        dataset_extra = dataset_extra.filter(\n",
    "            lambda x: len(x[\"text\"].split(\" \")) > 10\n",
    "            and len(x[\"text\"].split(\" \")) <= 1024\n",
    "            and x[\"idx\"] != 64707\n",
    "            and detect(x[\"text\"]) == \"en\"\n",
    "        )\n",
    "\n",
    "        dataset_extra = dataset_extra.filter(\n",
    "            lambda x: x[\"label\"] == IESTAHuggingFace._LABEL2ID_[effect]\n",
    "            and x[\"idx\"] not in idxes\n",
    "        ).shuffle(seed=seed)\n",
    "        dataset_extra = dataset_extra.select(range(limit - len(dataset)))\n",
    "        print(f\"{len(dataset_extra)} of extra\")\n",
    "        dataset = concatenate_datasets([dataset, dataset_extra])\n",
    "\n",
    "        print(f\"{len(dataset)} new length\")\n",
    "    print(f\"Return dataset {name} with {len(dataset)} \")\n",
    "    # dataset = dataset.map(lambda example, idx: {\"id\": idx, **example}, with_indices=True)\n",
    "\n",
    "    df = dataset.to_pandas().copy()\n",
    "    if profile:\n",
    "        report = ProfileReport(df=df, minimal=False)\n",
    "        report.to_file(f\"{ideology}_test_{500}_seed_{seed}\")\n",
    "\n",
    "    if save:\n",
    "        df.to_csv(f\"{ideology}_test_{500}_seed_{seed}.csv\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib = get_data(\"liberal\", save=True, profile=True).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons = get_data(\"conservative\", save=True, profile=True).to_pandas()\n",
    "# cons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generations(ineffective_argument: str, ideology: str):\n",
    "    result_dict = {}\n",
    "    for k, prompt_template in prompt_dict.items():\n",
    "        chat_prompt = ChatPromptTemplate.from_messages(\n",
    "            create_prompt_template(prompt_template.format(ideology=ideology))\n",
    "        )\n",
    "        llm_chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "        result_dict[k] = llm_chain.run(\n",
    "            ineffective_argument=ineffective_argument\n",
    "        )\n",
    "        result_dict[f\"len_{k}\"] = len(result_dict[k])\n",
    "        result_dict[f\"len_orig\"] = len(ineffective_argument)\n",
    "        # print(f\"'{result}'\\n\\n\")\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from os.path import exists\n",
    "\n",
    "\n",
    "def generate_args(\n",
    "    ideology: str,\n",
    "    out_file: str = \"../data/llms_out/\",\n",
    "    model_name: str = \"gpt3.5turbo\",\n",
    "    profile: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    out_file = f\"{out_file}{ideology}_{model_name}_2.jsonl\"\n",
    "\n",
    "    existing_indices = []\n",
    "\n",
    "    if exists(out_file):\n",
    "        _df = pd.read_json(path_or_buf=out_file, lines=True)\n",
    "        existing_indices = _df[\"idx\"].values.tolist()\n",
    "\n",
    "    filtered_dataset = get_data(\n",
    "        ideology, effect=\"ineffective\", limit=500, profile=profile, save=True\n",
    "    )\n",
    "    add_new_l = False\n",
    "    if len(existing_indices) > 0:\n",
    "        print(f\"filtering out existing indices ({len(existing_indices)})\")\n",
    "        filtered_dataset = filtered_dataset.filter(\n",
    "            lambda example: example[\"idx\"] not in existing_indices\n",
    "        )\n",
    "        print(f\"{filtered_dataset.num_rows} to go...\")\n",
    "        add_new_l = True\n",
    "\n",
    "    with open(out_file, \"a\") as file:\n",
    "        for datapoint in tqdm(filtered_dataset):\n",
    "            try:\n",
    "                promt_generated_dict = get_generations(\n",
    "                    datapoint[\"text\"], ideology\n",
    "                )\n",
    "                promt_generated_dict.update(datapoint)\n",
    "\n",
    "                nline = \"\\n\" if add_new_l else \"\"\n",
    "\n",
    "                file.write(f\"{nline}{json.dumps(promt_generated_dict)}\")\n",
    "                add_new_l = True\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"Failed to get a response for ID: {datapoint['idx']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate for ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_args(ideology=\"liberal\", profile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_args(ideology=\"conservative\", profile=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate for ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_args(ideology=\"liberal\", model_name=\"falconinstruct7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dataset(\"notaphoenix/debateorg_w_effect_for_liberal\", split=\"test\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in result_dict.items():\n",
    "    print(f\"\\n{k} - {prompt_dict[k]}\")\n",
    "    diff = Redlines(ineffective_argument, v)\n",
    "    display(Markdown(diff.output_markdown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "for ptype1, ptype2 in itertools.combinations_with_replacement(\n",
    "    prompt_dict.keys(), 2\n",
    "):\n",
    "    if ptype1 == ptype2:\n",
    "        continue\n",
    "    print(f\"\\n{ptype1} VS. {ptype2}\")\n",
    "    print(len(f\"{ptype1} VS. {ptype2}\") * \"-\")\n",
    "\n",
    "    diff = Redlines(result_dict[ptype1], result_dict[ptype2])\n",
    "    display(Markdown(diff.output_markdown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in result_dict.items():\n",
    "    for k,v  in result_dict.items():\n",
    "    print(f\"\\n{k} - {prompt_dict[k]}\")\n",
    "    diff = Redlines(ineffective_argument,v)\n",
    "    display(Markdown(diff.output_markdown))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iesta_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
