{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAIChat\n",
    "from langchain import LLMChain\n",
    "from langchain.text_splitter import CharacterTextSplitter,TokenTextSplitter\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "from ast import literal_eval\n",
    "import random\n",
    "import re\n",
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !source ../../../elbaff_iesta_venv/bin/activate\n",
    "# %pip install langchain\n",
    "# %pip install python-dotenv\n",
    "#%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "liberal_chat_prompt = ChatPromptTemplate.from_messages(create_prompt_template(prompt_dict[\"all\"].format(ideology =\"liberal\")))\n",
    "llm_chain = LLMChain(llm=chat, prompt=liberal_chat_prompt)\n",
    "result = llm_chain.run(ineffective_argument=\"If there was no Kryptonite, can Superman defeat the Silver Surfer?\")\n",
    "print(result)\n",
    "cons_chat_prompt = ChatPromptTemplate.from_messages(create_prompt_template(prompt_dict[\"all\"].format(ideology =\"conservative\")))\n",
    "llm_chain = LLMChain(llm=chat, prompt=cons_chat_prompt)\n",
    "result = llm_chain.run(ineffective_argument=\"If there was no Kryptonite, can Superman defeat the Silver Surfer?\")\n",
    "print(result)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new OpenAI instance\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "basic_str = \"Transform the following argument to an effective argument by maintaining the original length\"\n",
    "ideology_str = \"for readers with a {ideology} political ideology\"\n",
    "content_str = \"by preserving the content of the argument\"\n",
    "style_str = \"by only changing the style of the text\"\n",
    "\n",
    "prompt_dict = {\n",
    "    \"basic\": f\"{basic_str}:\",\n",
    "    \"ideology\": f\"{basic_str} {ideology_str}:\",\n",
    "    \"content\":  f\"{basic_str} {content_str}:\",\n",
    "    \"style\":f\"{basic_str} {style_str}:\",\n",
    "    \"ideology-content\": f\"{basic_str} {ideology_str} {content_str}:\",\n",
    "    \"ideology-style\": f\"{basic_str} {ideology_str} {style_str}:\",\n",
    "    \"all\": f\"{basic_str} {ideology_str} {content_str} and {style_str}:\",\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_prompt_template(prompt):\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(prompt)\n",
    "    human_template=\"{ineffective_argument}\"\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "    return [system_message_prompt, human_message_prompt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from iesta.machine_learning.huggingface_loader import IESTAHuggingFace\n",
    "\n",
    "def get_data(ideology, effect='ineffective', limit=500):\n",
    "    name:str = f'notaphoenix/debateorg_w_effect_for_{ideology}'\n",
    "    dataset: Dataset = load_dataset(name, split=\"test\")\n",
    "    dataset = dataset.filter(lambda x: x[\"label\"] == IESTAHuggingFace._LABEL2ID_[effect]).shuffle(seed=2062021)\n",
    "    \n",
    "    if len(dataset) > limit:\n",
    "        dataset = dataset.select(range(limit))\n",
    "    print(f\"Return dataset {name} with {len(dataset)} \")\n",
    "    dataset = dataset.map(lambda example, idx: {\"id\": idx, **example}, with_indices=True)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_generations(ineffective_argument:str, ideology:str):\n",
    "    \n",
    "    result_dict = {}\n",
    "    for k, prompt_template in prompt_dict.items():\n",
    "        \n",
    "        chat_prompt = ChatPromptTemplate.from_messages(create_prompt_template(prompt_template.format(ideology=ideology)))\n",
    "        llm_chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "        result_dict[k] = llm_chain.run(ineffective_argument=ineffective_argument)\n",
    "        #print(f\"'{result}'\\n\\n\")\n",
    "    return result_dict\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from os.path import exists\n",
    "\n",
    "def generate_args(ideology:str, out_file :str = \"../data/llms_out/\") -> pd.DataFrame:\n",
    "    out_file = f\"{out_file}{ideology}_gpt3.5turbo.jsonl\"\n",
    "    \n",
    "    existing_indices = []\n",
    "    \n",
    "    if exists(out_file):\n",
    "        _df = pd.read_json(path_or_buf=out_file, lines=True)\n",
    "        existing_indices = _df['id'].values.tolist()\n",
    "    \n",
    "    filtered_dataset = get_data(ideology, effect=\"ineffective\", limit=500)\n",
    "    add_new_l = False\n",
    "    if len(existing_indices) > 0 :\n",
    "        print(f\"filtering out existing indices ({len(existing_indices)})\")\n",
    "        filtered_dataset = filtered_dataset.filter(lambda example: example['id'] not in existing_indices)\n",
    "        print(f\"{filtered_dataset.num_rows} to go...\")\n",
    "        add_new_l = True\n",
    "    \n",
    "    with open(out_file, 'a') as file:\n",
    "\n",
    "        for datapoint in tqdm(filtered_dataset):\n",
    "            try:    \n",
    "                \n",
    "                promt_generated_dict = get_generations(datapoint[\"text\"], ideology)\n",
    "                promt_generated_dict.update(datapoint)\n",
    "\n",
    "                nline = \"\\n\" if add_new_l else \"\"\n",
    "\n",
    "                file.write(f\"{nline}{json.dumps(promt_generated_dict)}\")\n",
    "                add_new_l = True\n",
    "            except Exception as e:\n",
    "\n",
    "                print(e)\n",
    "                print(f\"Failed to get a response for ID: {datapoint['id']}\")   \n",
    "\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration notaphoenix--debateorg_w_effect_for_liberal-1efe322430b6ce3a\n",
      "Found cached dataset parquet (/home/elba_ro/.cache/huggingface/datasets/notaphoenix___parquet/notaphoenix--debateorg_w_effect_for_liberal-1efe322430b6ce3a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Loading cached processed dataset at /home/elba_ro/.cache/huggingface/datasets/notaphoenix___parquet/notaphoenix--debateorg_w_effect_for_liberal-1efe322430b6ce3a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-046e1b0d5f29e6aa.arrow\n",
      "Loading cached shuffled indices for dataset at /home/elba_ro/.cache/huggingface/datasets/notaphoenix___parquet/notaphoenix--debateorg_w_effect_for_liberal-1efe322430b6ce3a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-8558916e1ff496ae.arrow\n",
      "Loading cached processed dataset at /home/elba_ro/.cache/huggingface/datasets/notaphoenix___parquet/notaphoenix--debateorg_w_effect_for_liberal-1efe322430b6ce3a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-e7cd32d1f2938071.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return dataset notaphoenix/debateorg_w_effect_for_liberal with 500 \n",
      "filtering out existing indices (500)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.017568111419677734,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9032f555a20f4b81a4c1e936ccefac93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 to go...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "generate_args(ideology=\"liberal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration notaphoenix--debateorg_w_effect_for_conservative-8855d3de38b65ed6\n",
      "Found cached dataset parquet (/home/elba_ro/.cache/huggingface/datasets/notaphoenix___parquet/notaphoenix--debateorg_w_effect_for_conservative-8855d3de38b65ed6/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.029842138290405273,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 6,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b50df4846ade4967af1d467fdfe844bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return dataset notaphoenix/debateorg_w_effect_for_conservative with 500 \n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.021065235137939453,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 500,
       "unit": "ex",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0fa53a06e834227a622d45b6ad3aba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/500 [02:56<7:17:47, 52.85s/it] "
     ]
    }
   ],
   "source": [
    "generate_args(ideology=\"conservative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration notaphoenix--debateorg_w_effect_for_liberal-1efe322430b6ce3a\n",
      "Found cached dataset parquet (/home/elba_ro/.cache/huggingface/datasets/notaphoenix___parquet/notaphoenix--debateorg_w_effect_for_liberal-1efe322430b6ce3a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': \"Con Now I know full good and well that most of you reading this debate actually believe that .999... is equal to 1. I've seen a lot of Debates on the issue and frankly Con has generally done a horrible job supporting the position,not to say Pro is correct. There is no question in my mind that 1 and .999... recurring forever is not equal but entirely two different numbers and I will help you understand why. .999... = 1 is a false statement. Definitions .999... refers to .9 with recurring nines 1 refers to the real number, 1 = means is exactly equal to .999... is actually equal to .999... not one. Most people that contend 1 is = to .999... usually provide the below mathematical proof. Step 1) Let x = .999... Step 2) 10x = 9.999... (multiplying RHS and LHS by 10) Step 3) 10x - x = 9.999... - x (subtracting x from both sides) Step 4) 9x = 9 Step 5) x = 1 Conclusion .999... = 1 HOWEVER if you will notice step 4 is incorrect. It refers to 9x=9. This is wrong it actually equals 9x=8.999...1 or 8.99forever with a one at the end. Lets deal with finite to illustrate. Now many will contend that since the nines never end that you can not put a one at the end, however the one actually occurs FIRST then the 9's follow and work backwards to the decimal point. Step 1) Let x = .999 (just 3 of them, not infinite) Step 2) 10x = 9.99 (multiplying RHS and LHS by 10) Step 3) 10x - x = 9.99 - x (subtracting x from both sides) Step 4) 9x = 8.991 (there is always one 9 less than x and a 1 at the end) Step 5) x = .999 Conclusion .999 doesn't equal 1 The first logic most people refer to, and correctly so, is the concept that .999... is a theoretical number in which it is the number closest to one but still not 1. This is a wise individuals first thoughts when someone attempts to suggest that 1 is equal to .999... Attempts to prove this silly notion with mathematics, are being made and this is the point where most people sacrifice their intellect and simply accept that the Theoretical number .999... is equal to one. This is going to get complex, but if you think about it you will understand. Now here is the flaw in the algebraic examples. They are numbering systems that are based on 10. Most of you only have conceptualized numbering systems based on 10 but it doesn't have to be based on 10. It could just as easily based on lets say 8, this is where you must really use your intellect to understand. If the numbering system was 8 lots of things remain the same, however a few important things change, namely decimals. Now lets say we have the number system of 8. 1/8 now would equal .1 and 1/4 would equal .2. 1/3 would now equal .2666.. as opposed to .333... This may sound silly but you have to understand that decimals simply represent the numbering system chosen and my opponent is using 10. Now regardless of which numbering system you use whether it is 8 or 10 one third (1/3) will be equal to (1/3) however the way the number is represented in decimal form changes. So .375 of the 8 numbering system equals .333... in the 10 numbering system. To help you better understand numbering systems our calendar months per year is based on 12. If you wanted to represent half a year it would be 6 months. Represented using a numbering/numeral system based on 10 the decimal representation of half would be .5. A 12 numbering system would represent the decimal of .6. A 8 numbering system would have 6 months represent .4 in decimal format. All systems would still represent the 6 months as 6/12 in fraction representation. So if you can see it is simply the decimal representation of the fraction that varies according to the numbering/numerical system. <URL>... The error with the algebraic expressions is the decimal system based on 10. The 10 system is the most widely used system in the world today, and many find it difficult to understand other systems because it is like learning math all over.\",\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dataset(\"notaphoenix/debateorg_w_effect_for_liberal\", split=\"test\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in result_dict.items():\n",
    "    print(f\"\\n{k} - {prompt_dict[k]}\")\n",
    "    diff = Redlines(ineffective_argument,v)\n",
    "    display(Markdown(diff.output_markdown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "for ptype1, ptype2 in itertools.combinations_with_replacement(prompt_dict.keys(), 2):\n",
    "    if ptype1 == ptype2:\n",
    "        continue\n",
    "    print(f\"\\n{ptype1} VS. {ptype2}\")\n",
    "    print(len(f\"{ptype1} VS. {ptype2}\")*\"-\")\n",
    "\n",
    "    diff = Redlines(result_dict[ptype1],result_dict[ptype2])\n",
    "    display(Markdown(diff.output_markdown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in result_dict.items():\n",
    "    for k,v  in result_dict.items():\n",
    "    print(f\"\\n{k} - {prompt_dict[k]}\")\n",
    "    diff = Redlines(ineffective_argument,v)\n",
    "    display(Markdown(diff.output_markdown))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iesta_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
