{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Training Data\n",
    "%pip show nlpaf\n",
    "%pip install  -e ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elba_ro/repos/github/conf22-style-transfer/iesta_venv/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "from  iesta.machine_learning.dataloader import load_features_df\n",
    "import iesta.properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.dataset_dict import DatasetDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016066551208496094,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading builder script",
       "rate": null,
       "total": 4314,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc354630215849a6bb6d64d30aae15b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.31k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.026427507400512695,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading metadata",
       "rate": null,
       "total": 2166,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c6ed8accab43a5bc138dd8b021e5e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.024593591690063477,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading readme",
       "rate": null,
       "total": 7590,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e85ff93a68f64bb8a9f62c162eb8374c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elba_ro/repos/github/conf22-style-transfer/iesta_venv/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imdb/plain_text to /home/elba_ro/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.024415254592895508,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading data",
       "rate": null,
       "total": 84125825,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be6e4e06ca38456fbb4b1f00f675d9cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01254129409790039,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Generating train split",
       "rate": null,
       "total": 25000,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ac572cf18542c9bae167707cb0bdba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.025177717208862305,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Generating test split",
       "rate": null,
       "total": 25000,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fab0a75e49248c6aa72489ba5b1dcbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015985488891601562,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Generating unsupervised split",
       "rate": null,
       "total": 50000,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c287344d8b194022b2725e33aac85c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imdb downloaded and prepared to /home/elba_ro/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010835409164428711,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "166581ac495644b8a24dea06eaac20f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imdb = load_dataset(\"imdb\")\n",
    "type(imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Included effects are: ['effective' 'ineffective' 'provocative']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method IndexOpsMixin.tolist of Index(['id', 'debate_id', 'p_name', 'effects', 'effect_count', 'top_effect',\n",
       "       'effect', 'category', 'round', 'argument',\n",
       "       ...\n",
       "       'hedge_N_ratio', 'hedge_N_words', 'hedge_dominant', 'toxicity_dominant',\n",
       "       'toxicity_neutral', 'toxicity_neutral_count', 'toxicity_neutral_ratio',\n",
       "       'toxicity_toxic', 'toxicity_toxic_count', 'toxicity_toxic_ratio'],\n",
       "      dtype='object', length=854)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from iesta.machine_learning.dataloader import IESTAData\n",
    "def get_training(feature_dfs, ideology, evaluation_classfier_data_flag= IESTAData._WITHOUT_EVAL_CLASSIFIER, discard_okay: bool = True):\n",
    "    _, feature_dfs = load_features_df()\n",
    "\n",
    "    training = feature_dfs[ideology]\n",
    "    print(len(training))\n",
    "\n",
    "    if evaluation_classfier_data_flag == IESTAData._ONLY_EVAL_CLASSIFIER_:\n",
    "        training = training[training['is_for_eval_classifier'] == True]\n",
    "        print(len(training))\n",
    "    elif evaluation_classfier_data_flag == IESTAData._WITHOUT_EVAL_CLASSIFIER:\n",
    "        training = training[training['is_for_eval_classifier'] == False]\n",
    "        print(len(training))\n",
    "    if discard_okay:\n",
    "        training = training[training[\"effect\"]!=\"okay\"]\n",
    "    print(f\"Included effects are: {training.effect.unique()}\")\n",
    "    return training\n",
    "\n",
    "liberal_training = get_training(feature_dfs, \"liberal\")\n",
    "liberal_training[\"binary_effect\"].value_counts()\n",
    "liberal_training.columns.tolist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "unique_debates = liberal_training.debate_id.unique()\n",
    "\n",
    "training_debates, testval_debates = train_test_split(unique_debates, test_size=0.3,\n",
    "                 random_state=42,\n",
    "                 shuffle=True)\n",
    "\n",
    "def _add_splits(row, training_debates, testval_debates):\n",
    "    if row['debate_id'] in training_debates:\n",
    "        row[\"classifier_split\"] = \"train\"\n",
    "    elif row['debate_id'] in testval_debates:\n",
    "        row[\"classifier_split\"] = \"test\"\n",
    "    else:\n",
    "        assert False\n",
    "    return row \n",
    "\n",
    "liberal_training = liberal_training.apply(_add_splits, args=(training_debates,testval_debates, ),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liberal_training.classifier_split.value_counts()\n",
    "import pandas as pd\n",
    "\n",
    "pd.crosstab(liberal_training.classifier_split, liberal_training.binary_effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "def filter_cols(cols):\n",
    "    prefices = [\"mpqa\", \"hedge\", \"emotion\", \"toxicity\", \"mpqa\", \"empath\"]\n",
    "    keep = [c for c in cols if c.split(\"_\")[0] in prefices and not c.endswith(\"_count\")]\n",
    "    return keep\n",
    "\n",
    "def filter_feature_df(df: pd.DataFrame, class_col: str = \"effect\", split_col: str = \"classifier_split\") : # binary_effect\n",
    "    feature_cols = filter_cols(df.columns.tolist())\n",
    "    df_ = df[feature_cols+[class_col, split_col]].copy()\n",
    "    numeric_cols = df_.select_dtypes(include=np.number).columns.tolist()\n",
    "    df_ = df_[numeric_cols+[class_col, split_col]]\n",
    "    \n",
    "    df_.fillna(0.0, inplace=True)\n",
    "\n",
    "    # transform y to numeric\n",
    "    le = LabelEncoder()\n",
    "    le.fit(df_[class_col])\n",
    "    df_[class_col] = le.transform(df_[class_col])\n",
    "    #X = df_.values\n",
    "    #y_str = df[class_].values\n",
    "\n",
    "\n",
    "    #def to_numeric(x):\n",
    "    #    return 0 if x == \"ineffective\" else (1 if x == \"effective\" else 2)\n",
    "    #y = [to_numeric(x) for x in y_str]\n",
    "    \n",
    "    return df_, numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n",
    "list(le.classes_)\n",
    "le.transform([\"amsterdam\", ])\n",
    "le.in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "_df, numeric_cols = filter_feature_df(liberal_training, class_col = \"binary_effect\", split_col = \"classifier_split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df.binary_effect.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlpaf.ml.feature_based import FeatureBasedML\n",
    "\n",
    "trainer = FeatureBasedML(y_col=\"binary_effect\", dataset=_df, split_label_name=\"classifier_split\", training_cols=numeric_cols, remove_outliers=False, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = trainer.train_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfold_results = trainer.train_xfold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfold_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfold_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_nooutliers = FeatureBasedML(y_col=\"binary_effect\", dataset=_df, split_label_name=\"classifier_split\", training_cols=numeric_cols, remove_outliers=True, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfold_nooutlier_results = trainer_nooutliers.train_xfold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlpaf.ml.feature_based import FeatureBasedML\n",
    "\n",
    "trainer_ensemble = FeatureBasedML(y_col=\"binary_effect\", dataset=_df, split_label_name=\"classifier_split\", training_cols=numeric_cols, remove_outliers=True, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlpaf.ml import traditional_trainer as tt\n",
    "\n",
    "results = tt.train_ensemble(trainer_ensemble.X_train, trainer_ensemble.X_test, trainer_ensemble.y_train, trainer_ensemble.y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "          precision    recall  f1-score   support\n",
    "\n",
    "           0       0.06      0.12      0.08       563\n",
    "           1       0.95      0.91      0.93     11554\n",
    "\n",
    "    accuracy                           0.87     12117\n",
    "   macro avg       0.51      0.51      0.50     12117\n",
    "weighted avg       0.91      0.87      0.89     12117\n",
    "\n",
    "\n",
    "time: 2222m 7.3s\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iesta_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
