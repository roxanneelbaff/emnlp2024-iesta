{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet  langchain langchain-core langchain-community langchain-openai\n",
    "\n",
    "!pip install -qU langchain-openai\n",
    "!pip install -qU langchain-mistralai\n",
    "!pip -q install transformers -U \n",
    "!pip -q install langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install -U tenacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dotenv was False\n",
      "dotenv was False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'iesta.evaluator.llm_ideology' from '/Users/roxanneelbaff/Documents/projects/github/eacl2024-iesta/iesta/evaluator/llm_ideology.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import getpass\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from iesta.evaluator import llm_ideology\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "import iesta\n",
    "import iesta.evaluator.llm_ideology\n",
    "\n",
    "\n",
    "import importlib\n",
    "\n",
    "importlib.reload(iesta)\n",
    "importlib.reload(iesta.evaluator.llm_ideology)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"gpt-3.5-turbo\", \"meta-llama/Llama-2-7b-chat-hf\"] #[\"gpt-4\",\"open-mixtral-8x7b\",]# \"gpt-3.5-turbo\", \"meta-llama/Llama-2-7b-chat-hf\"]\n",
    "ideologies = [\"None\"]\n",
    "prompt_types = [\"role_play_prompt_4\"]#, \"imagine\", \"role_play_prompt_1\", \"role_play_prompt_2\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary experiment: Prompt 2 generation from prompt1 for role play \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_ideology.get_prompt2_samples(\n",
    "    model_lst = models,\n",
    "    ideology_lst = ideologies,\n",
    "    temperature = 0.7,\n",
    "    repetitions = 50,\n",
    "    root = \"data/\",\n",
    ")\n",
    "\n",
    "prompt2_top_df = llm_ideology.get_top_prompt2(\"data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAIN EXPERIMENT - RUN PEW Quiz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "from tenacity import retry, stop_after_attempt, before_log, wait_fixed\n",
    "\n",
    "logging.basicConfig(stream=sys.stderr, level=logging.INFO)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def run_pew_all_settings():\n",
    "    repititions = 30\n",
    "    for prompt_type in tqdm(prompt_types):\n",
    "        for model_name in tqdm(models):\n",
    "            for ideology in tqdm(ideologies):\n",
    "                if ideology == \"None\":\n",
    "                    continue\n",
    "                try:\n",
    "                    run_tests(model_name, repititions, ideology, prompt_type, )\n",
    "                except Exception as e:\n",
    "                    print(model_name, ideology, prompt_type,)\n",
    "                \n",
    "@retry(stop=stop_after_attempt(5), wait=wait_fixed(2))#, before=before_log(logger, logging.INFO))\n",
    "def run_tests(model_name, repititions, ideology, prompt_type, ):\n",
    "    print(\"MODEL: \", model_name)\n",
    "    print(\"IDEOLOGY: \", ideology)\n",
    "    print(\"PROMPT TYPE: \", prompt_type)\n",
    "    pew_for_llm = llm_ideology.PEWQuizForLLM(model_name,\n",
    "                                            repeat_quiz = 3,\n",
    "                                            ideology_key = ideology,\n",
    "                                            prompt_type = prompt_type,\n",
    "                                            root = \"data/\",\n",
    "                                            temperature = 0.0)\n",
    "    # 80/3 per iteration\n",
    "    pew_for_llm.repeat_pew_quiz()\n",
    "    print(\"---------- FINISHED---------------\") \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "run_pew_all_settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'ideology',\n",
       " 'batch',\n",
       " 'iteration',\n",
       " 'prompt',\n",
       " 'original_effectiveness',\n",
       " 'original_clarity',\n",
       " 'original_feedback',\n",
       " 'rewrite1_effectiveness',\n",
       " 'rewrite1_clarity',\n",
       " 'rewrite1_consistency',\n",
       " 'rewrite1_feedback',\n",
       " 'rewrite2_effectiveness',\n",
       " 'rewrite2_clarity',\n",
       " 'rewrite2_consistency',\n",
       " 'rewrite2_feedback',\n",
       " 'favorite',\n",
       " 'original_effectiveness_feedback',\n",
       " 'original_clarity_feedback',\n",
       " 'rewrite1_effectiveness_feedback',\n",
       " 'rewrite1_clarity_feedback',\n",
       " 'rewrite1_consistency_feedback',\n",
       " 'rewrite2_effectiveness_feedback',\n",
       " 'rewrite2_clarity_feedback',\n",
       " 'rewrite2_consistency_feedback',\n",
       " 'original_consistency']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(\"data/liberal_llm_based_eval1-3.csv\").columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 min 51 sec. for 4 iterations + 1 min 50s\n",
    "# 10 iter: 3 59.1 s\n",
    "# 6 : 2m 41.8s +  2m\n",
    "\n",
    "# gpt 4 : 30 iterations 12.365 minutes. cost: 3.02 $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "pew_result_file_lst = glob.glob(\"data/llm_ideology/pew_quiz_results/*.csv\")\n",
    "#assert len(pew_result_file_lst) == 780"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pew_result_file_lst[0].split(\"_\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
